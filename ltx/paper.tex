
\documentclass[smallcondensed, final]{svjour3}
% \documentclass[twocolumn, final]{svjour3}


\usepackage[square,numbers]{natbib}
\usepackage{amsmath, epsfig}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{easybmat}
\usepackage{footmisc}
\usepackage[usenames,dvipsnames]{color}
\usepackage{subfig}
\renewcommand\algorithmiccomment[1]{// \textit{#1}}
\newcommand{\ignore}[1]{}
\newcommand{\comment}[1]{}
\newcommand{\frank}[1]{\textcolor{red}{\textsf{\emph{\textbf{\textcolor{blue}{#1}}}}}}
\newcommand{\willie}[1]{\textcolor{green}{\textsf{\emph{\textbf{\textcolor{green}{#1}}}}}}
\DeclareMathOperator*{\argmax}{arg\,max}


\begin{document}

\title{Unsupervised Detection and Tracking of Multiple Objects with Dependent Dirichlet Process Mixtures}
\author{Willie Neiswanger and Frank Wood}
\maketitle
\mbox{}

\abstract{Main points.}



\section{Introduction}
% % \subsection{Topic Overview and Terminology}

We define the task of automated detection and tracking of arbitrary objects in videos to be the fulfillment of three actions: determining the spatiotemporal regions of a video that constitute objects (`extraction'), finding the positions and/or shapes of distinct objects (`localization'), and maintaining the identities of the detected objects over time (`tracking'). An ability to carry out these tasks in an automated manner is useful for many fields that make use of video data, including robotics, video surveillance, time-lapse microscopy, and video summarization. Algorithms able to be applied to a variety of objects and video types are particularly desirable. This paper serves to introduce a new technique involving the use of dependent Dirichlet processes mixture models, which we hope will provide the foundation for a class of unsupervised algorithms that can detect and track arbitrary objects in a wide range of videos.

Research in the past ten years involving the development of methods for general detection and tracking of objects has tended to focus on one of either extraction, localization, or tracking; attempts to integrate all three tasks and produce a system aimed at multiple arbitrary objects and diverse video types was not often a primary focus, or was attempted in an ad-hoc manner to test or apply a more specific technique. Research aimed at developing methods to accomplish the three tasks in a cohesive manner has been pursued more recently to produce algorithms that can, in a fully automated way, accomplish unsupervised detection and tracking--or in short, that can determine the spatiotemporal region of a video occupied by each distinct object. A widely applied, though unsophisticated, example of general detection and tracking is the technique of blob tracking. Blob tracking typically refers to methods that perform extraction and localization of objects at each frame independently--for example, by assuming that pixels representing an object are distinguishable from pixels representing the background via color, motion, or texture information, and segmenting the extracted data at each frame into ``blobs'' to accomplish localization. A number of tracking techniques based on finding blobs in consecutive frames of a video with similar locations and appearances have been attempted to maintain the identity of detected blobs over time. The main challenges, however, associated with blob tracking involve the difficulty of segmenting neighboring or occluding objects in a given frame \cite{zhao2004tracking} and preventing multiple tracked blobs from collapsing onto and tracking a single object \cite{vermaak_2003}. These difficulties stem from the typical blob tracker's need to perform detection via segmentation on individual frames and lack of a principled way to perform tracking while resisting the collapse of independent tracks.

This paper presents the use of a Bayesian nonparametric mixture model for general, unsupervised multiple object detection and tracking. This method is intended to provide a widely applicable way to detect arbitrary objects in videos--particularly in cases where frame-by-frame segmentation is difficult or video quality is low and extraction is noisy--and better maintain the isolation of independent objects during tracking. Specifically, this work describes how a Dirichlet process mixture model can accomplish these goals when it is applied to data that may be extracted in a general manner from videos. We will demonstrate how to extract data from objects in videos and how this generative model allows for automated detection and tracking. The model is first presented in an abstract form, to allow it to be used with arbitrary extraction procedures and object appearance representations that may be desired in future studies. We then formulate a specific model for data gained during a basic extraction procedure and describe inference algorithms that allow object localization and tracking to be carried out. We demonstrate our implementation on a number of synthetic and benchmark datasets and describe and compute performance metrics to quantify results. 


\subsection{Related Work}
\label{sec:relatedwork}


Related work involves studies related to extraction procedures, localization techniques, single and multiple object tracking methods, and combination methods for joint object detection and tracking.

Extraction is related to the tasks of foregound segmentation, background modeling, motion detection, and appearance feature extraction. In general, these methods aim to extract features from regions of a video frame that are considered objects or are distinguished in some way from the background of a scene. Areas of research in this field include attempts to handle situations where the background displays motion or other dynamic characteristics, reduce noise caused by the false extraction of backgrounds, handle abberant image effects present in some videos, and operate accurately on scenes involving illumination changes. These procedures can usually be classified as either model-based methods or heuristic-based methods. Model-based methods emphasize the detection of shapes of regions containing objects accurately and reducing background noise. For example, foreground extraction has been robustly carried out by modeling each pixel over time in order to infer a probability distribution over pixel values; if the pixel takes on a very unlikely value at a future time, it is marked as a foreground pixel at that point \citep{stauffer_1999, elgammal_2000, elgammal_2002}. Note that these techniques treat each pixel independently; this has a tendency to cause errors in foreground detection, resulting in fragmentation of foreground images (which is problematic as many of these methods intend to blob the foreground objects after detection in order to perform segmentation and locate objects' centroids). Heuristic-based methods perform extracting by looking for temporal changes in pixel value characteristics. These methods are often less accurate than model-based methods, but also far less computationally expensive. Frame differencing and background subtraction are two such methods, and involve comparing pixel values, or groups of pixel values, between two images, to detect a change surpassing some threshold. The goal of these techniques is to find locations in a scene that display motion, and to deem these moving areas the foreground of a scene.  Often, background subtraction refers to comparing an image containing targets with an image of the background without targets or with some model of the background that is learned as the video progresses (in these cases, objects that do not move throughout a scene will be treated as the background), while frame or image differencing refers to comparing two subsequent images in a video. Frame differencing has been used as the sole extraction method for object localization or tracking schemes with success \citep{pece_2002, beleznai_2006, chu_2007}, and also as a secondary data extraction method to help improve the accuracy of object tracking schemes \citep{perez_2002}. In addition to the two techniques mentioned above, there exist other ways of extracting movement from an image, such as through techniques involving optical flow (a calculation based on the `generalized gradient model' of an image, which attempts to capture the speed and direction of movement over areas in the image) \citep{horn_1981, bobick_2001}. In \citep{black_2000}, a motion detection and object tracking method is developed based on the relative motion of moving objects' boundaries, which are again found through analysis of optical flow.

Localization is closely related to the topics of object detection, position-finding, segmentation, and target recognition. These methods are mainly concerned with finding the positions and/or shapes of objects in a given video frame, or with discerning between objects similar in position or appearance. There has been a focus in this field on developing techniques for representing an object's appearance, both to allow for localization of objects on a per-frame basis and to allow for consistent localization of a given object over time. Research into the unsupervised detection of arbitrary objects (which we define as methods that accomplish both extraction and localization) has included work involving image segmentation, parsing, and pixel clustering \willie{must cite here}. These topics usually relate to the automated partitioning of images into meaningful regions. Note that the popular phrase `object detection' is also typically used to refer to procedures that carry out extraction and localization; however, it is often reserved to denote methods that are tailored to find the position of specified objects or object types (as opposed to arbitrary object detection), or to denote supervised methods, where extraction is peformed implicitely during searches for specified objects and localization procedures.

Tracking is related to the topics of video tracking, data association, filtering, point matching, and `tracking of non-rigid objects'. In general, these tasks aim to maintain the localization of the position and/or shape of an object over time. Areas of interest in this field involve developing metrics for judging the similarity between two object representations (for example, potential representations of the same object at adjacent frames in a video), techniques that provide a way to conduct efficient searches over a frame for regions that are similar to a given object, and methods for predicting the future position or appearance of an object. 

Methods for tracking single, non-rigid, objects are numerous. Color, for example, has long been used for tracking. These techniques often operate by modeling an object with some color-based appearance model, and using this model to find the object in subsequent frames. One classic approach involves modeling the color of a target (in particular, a distribution over the hue-saturation space of a target region) with a Gaussian mixture model (GMM), and choosing subsequent target positions by searching surrounding areas for regions that yield similar GMMs. Furthermore, the GMM is often allowed to adapt over time, and slowly change to model changes in lighting or other smooth time-based variations in the target's color \citep{raja_1998, mckenna_1999, jepson_2003}. Additionally, color has been successfully applied as the feature in a technique known a the mean-shift procedure (or the kernel-based method) for tracking \citep{comaniciu_2003, perez_2002, nummiaro_2003, lee_2011}. This procedure--a derivation and overview of which can be found in \citep{fukunaga_1975, cheng_1995}--optimizes the search for the region in a frame which is most similar to a target region in the previous frame. It involves an iterative algorithm that repeatedly shifts each data point to the weighted average of data points in its neighborhood; it has been proven that this process converges for each data point. Additionally, the process (in particular, the specification of the neighborhood and the weight-distribution when calculating the weighted mean of nearby data points) is generalized so that multiple kernels can be specified and used to allow for varied clustering behaviors. It can be shown that, through this procedure, each data point becomes associated with a local point of high density (dependent upon the underlying weight distribution specified by the kernel) which naturally allows for clustering \citep{cheng_1995}. The mean shift algorithm has been implemented successfully to allow for a sort of kernel-based object tracking \citep{comaniciu_2003, comaniciu_1999, comaniciu_2000}.

In addition to single object tracking, a great deal of research in this field over the past decade has focused on the development of algorithms to track multiple objects simultaneously. There has been a particular emphasis on developing ways to deal with problems such as object occlusions (where one object blocks another from the view of a video camera), complex object interactions, object coalescence (where one object is tracked by multiple times/redundantly), objects with similar appearances, variable (and potentially high) numbers of objects, and objects that enter and exit a field of view at different times \willie{cite paper(s) for each of these}.

In the past five years, there has been a push for methods that incorporate detection and tracking of multiple objects to allow for full systems capable of autonomously handling real-world video datasets.
\willie{Describe tracking by detection, more on blob tracking, the other few methods i've found for automated joint detection and tracking}




% Others have attempted to abstract this work with a non-parametric color modeling approach based on kernel density estimation, which does not assume a specific underlying distribution (such as the Gaussian mixture in the previous case) and instead converges to reasonable distribution that depends on the data \citep{elgammal_2001}. 


% Extraction 


% 

% The mean-shift procedure (also known as `kernel-based object tracking') attempts to provide a robust way for non-rigid objects to be tracked. This method is beneficial because it optimizes the search for the `next' position of an object (i.e. the search for the region in a frame which is most similar to a target region in the previous frame). This procedure--a derivation and overview of which can be found in \citep{fukunaga_1975, cheng_1995}--involves an iterative algorithm that repeatedly shifts each data point to the weighted average of data points in its neighborhood; it has been proven that this process converges for each data point. Additionally, the process (in particular, the specification of the neighborhood and the weight-distribution when calculating the weighted mean of nearby data points) is generalized so that multiple kernels can be specified and used to allow for varied clustering behaviors. It can be shown that, through this procedure, each data point becomes associated with a local point of high density (dependent upon the underlying weight distribution specified by the kernel) which naturally allows for clustering \citep{cheng_1995}. The mean shift algorithm has been implemented successfully to allow for a sort of kernel-based object tracking \citep{comaniciu_2003, comaniciu_1999, comaniciu_2000}. Given the current position of an object at a given frame, the goal of tracking is often to find a nearby position in the next frame that has the most similar distribution over some common set of features. Usually this must be done through an exhaustive search, comparing the similarity of distributions at each nearby position with that at the current position. However, if a certain type of `isotropic kernel' (mean-shift kernel) known as the Bhattacharyya coefficient is chosen as a similarity metric between the feature distributions at two positions, it creates a smooth function where gradient descent techniques can be used to quickly converge upon an optimal subsequent position without using an exhaustive search. Many papers are concerned with applying this kernel-based object tracking scheme to different sets of features or with different kernels. Additionally, the scheme has been attempted with an adaptive kernel whose shape, scale, and orientation is influenced by a target being tracked \citep{wang_2004}, with a kernel adjusted by the estimated centroid of a tracked target \citep{mehmood_2009}, and with a heirarchical version of the mean-shift procedure \citep{dementhon_2002}. 

% Some have tried to extent single object tracking schemes to tracking multiple targets; at the most basic level, this involves initializing multiple targets and running an instance of (single) VOT for each, either simultaneously or in succession \citep{perez_2002}. Furthermore, if performed simultaneously, the tracking of each target can be made dependent upon characteristics of other targets (such as their proximity) to resolve errors and improve tracking (such as those caused by the incorrect merging of two targets) \citep{khan_2004, vermaak_2003} . 




% %  ------------------------------------------------------------------------------------
% % Localization-focused. Clustering methods, segmentation methods, object appearance models and representations (allows for consistent object localization over time).

% % Methods for tracking single objects in videos: filtering, similarity metrics / localization (and how they play a role), mean shift procedures.

% % extensions to multiple visual object tracking. problems with multiple independent single object trackers (coalesence, object interaction, object occlusion, objects entering and leaving scene [and, relatedly, variable numbers of objects]).
% % Multiple object tracking attempts at overcoming various the various challenges specified above (include nonparametric varmaak/doucet for maintaining multi-modality here?---sure, and then mention again later with fox's work?)
% % End with description of initialization problem.

% % Methods for detection and tracking of objects. Heuristic initialization at beginning and then carrying out above methods (problems with varying numbers of obects / objects entering or leaving scene). Supervised learning of specific targets to allow for initialization and provide better localization throughout. Various research has been towards integrating unsupervised extraction and localization into multiple object tracking methods. Particle filters with integrated detection (at least two of these). Introduce concept of blob tracking and define basic algorithm (some manner of extraction, independent/per-frame localization, tracking; note that each object is represented in each frame as a low-dimensional point [or higher dimensional thing like region/outline with sophisticated blob/contour detectors like bramble and others]). The key thing characterizing a blob detector is that the position of each object (or the spatially separate region associated with each object) is assigned/fixed at each frame without the aid of tracking (ie if localization/segmentation can be carried out at each frame independently), and tracking is done with this data. Schemes often are simply segmentation of extraction data into spatially disjoint blobs at each frame (where blobs  are not modeled in any principled manner), returning the position or region associated with each object at each frame.

% % When the state of objects (for example, position and/or other characteristics) can be found independently at each frame, well established methods in the field known as Multiple Target Tracking (MTT) can be carried out to perform tracking of multiple objects. Two classic algorithms for MTT are known as MHT and JPDA. In general, MTT methods are useful for procedures that can represent each object at each frame as a point or region without localization information from this object at a previous frame (which requires the use of tracking). MTT in the literature is often quite separate from fields of computer vision involving object detection and tracking research. In general, the situation arises as video data without modification is very high dimensional (discussion of difference between MTT and detection methods here?).


% % Methods like blob tracking which attempt to find the state of each object independently at each frame have a hard time dealing with occlusion and other complexities (except for sophisticated blob trackers like bramble which isn't really blob tracking). Attempts have been made to perform time dependent clustering on extraction data (look at cluster tracking paper for good def), to carry out joint localization and tracking; this has been refered to as cluster tracking. Work of Pece, MTT of multiple targets work(?). The work presented in this paper most closely resembes the topics listed here. Does cluster tracking on whichever extraction data, using a time dependent Dirichlet Process mixture model (to better estimate and maintain the number of targets, as was shown to be effective in vermaak and fox), which takes the form of a sequence of clusters, where the objects' appearances are modeled in a principled manner by the cluster component distributons, and the tracking behavior is modeled by time-dependencies between adjacent elements in the cluster-sequences.
% %  ------------------------------------------------------------------------------------




% \subsection{Contributions}

% This paper applies a time dependent Dirichlet process mixture to model data gained during extraction procedures, provides background as to how this Bayesian nonparametric model allows for the incorporation of prior knowledge about object appearances and motion behaviors, describes methods for unsupervised detection and tracking of arbitrary objects in terms of Bayesian inference procedures, and presents a specific implementation of the model and inference procedures to carry out detection and tracking on a number of target-types in different settings.

% % List here the benefits of this idea that should be discussed/listed:
% This paper is heavily model-based, which differs from many papers in the field of multiple object tracking that do not explicitely describe models which underlie various procedures that are carried out. Solutions in many papers are, instead, algorithm oriented. The model-based approach allows rigor to be kept as inference procedures are developed. Furthermore, development of inference procedures are performed separately from development of the model, and various well studied inference and approximate inference methods can be carried out to achieve algorithmic results that are (in some cases) known to be optimal.
% %treated separately, which allows for multiple methods to be carried out in a unified framework and   these solutions are  but provide algorithm-based presentations.
% %	This way allows rigor to be kept as inference procedures are developed.
% %	Furthermore, inference is treated separately; this allows multiple detection and tracking algorithms to be developed (in a well-formed/rigorous fashion) by developing different inference techniques. Also, inference on graphical models is a well-structured field and allows for use of techniques in this field as well as better interpretations of the resulting detection/tracking algorithms.
	
% % (related to previous?) models objects as distribution-components (of a mixture model) as opposed to point-targets or targets with noise that adheres to some distribution (is this truly a distinction and is it good?)



% This idea presents a framework which allows for time-varying models of objects' appearances and motion behaviors to be incorporated, and is formulated in a Bayesian manner that allows distributions underlying the parameters of these models to be specified in a principled manner. It has been shown to work successfully in conjunction with simple (noisy) extraction methods. More robust, expensive, or in general accurate methods can be used for further accuracy. This method is able to handle the appearance and disappearance of objects from a scene, variable numbers of objects, cases where objects show time varying changes in behavior, and cases of occlusion.

% Section two of this paper provides an overview of the model and describes how aspects can be modified for for modeling object appearance and motion behavior. Section three presents a specific implementation of the model and a number of inference procedures for carrying out object detection and tracking. Section four describes a number of experiments carried out with this implementation. Section five describes results of these experiments, comments on further directions of study, and provides concluding remarks.













\section{Model Overview}
\label{sec:modeloverview}
%
This section provides details on the data to be modeled, the Dirichlet process mixture (DPM) model, and the dependent Dirichlet process mixture (DDPM) model. We make explicit the way in which characteristics of video objects can be represented by elements of these models. We formulate the model in a sufficently abstract manner to allow for implementations tailored for specified objects in future studies.




\subsection{Data}
\label{sec:data}
%
We defined the task of extraction to be the unsupervised determination of which spatiotemporal regions of a video constitute objects, and outlined a number of extraction methods in Section~\ref{sec:relatedwork}. Extraction allows us to generate a collection of data points. Each data point can be thought of as a hypothesis about, or noisy version of, the representation of an object; a collection of hypotheses for a given object is assumed to be distributed in some way around the true object representation. A video of multiple objects is therefore assumed to yield data that contains multiple groups of object hypotheses with similar characteristics (i.e. clusters of similar hypotheses).

Determining the regions of each video frame that contain objects allows us to generate these object hypotheses; namely, we let each hypothesis be a description of the pixel data over an area centered at a point in the extracted region. We can, for example, let each pixel in the extracted region be a hypothesis, and let the features of the representation consist of the spatial position of the pixel. An extraction dataset of this form is comprised of the three-dimensional points $\bold{x} = ( x_{1}, x_{2}, t ) \in \mathbb{R}^{2} \times \mathbb{Z}_{+}$, where $x_{1}$ and $x_{2}$ are the two spatial dimensions of a pixel contained in an extracted region in a given frame, and $t$ is the discrete time index of the frame. These three features are always available once regions containing objects are discerned, and are useful features for modeling (as position information is key for reasonable tracking and time information is necessary for maintaining the temporal dependencies in the data).

Examples of additional features that could potentially be extracted from object-regions in video frames include color information, pixel intensity values, feature point (such as corner, shape, or edge) locations or spatial characteristics, texture representations, transform calculations (such as results of hough or fourier transforms), and quantiative region descriptions derived from image segmentation procedures; the main idea is to choose features which are able to be easily extracted or computed, capture variability in the appearance of objects, are applicable to a wide variety of object types, and can be represented by distributions that allow for tractable inference (discussed further in Section~\ref{sec:inference}).

We will use the phrase ``baseline'' three-dimensional extraction data to refer to the features $\bold{x} = ( x_{1}, x_{2}, t ) \in \mathbb{R}^{2} \times \mathbb{Z}_{+}$ of pixels present in regions containing objects. These data are distributed roughly as worm-like shapes that follow the paths of individual objects as they move, when they are plotted in three dimensions (where we let the frame number $t$, our discrete representation of time, ascend the vertical axis and the spatial positions occupy the horizontal axes). An example of data with such a distribution can been seen in Figure~\ref{fig:baselinedata}. The data consists of the three-dimensional baseline features and was gained by extraction peformed on a 50 frame sequence showing five ants moving across the video scene.

% \begin{figure}[h]
%   \centering
%   \subfloat[]{\label{fig:baselinedata1}\includegraphics[width=0.33\textwidth]{../img/pets2001_data_1.png}} 
%   \subfloat[]{\label{fig:baselinedata2}\includegraphics[width=0.33\textwidth]{../img/usbh_data_3.png}} 
%   \subfloat[]{\label{fig:baselinedata3}\includegraphics[width=0.33\textwidth]{../img/traffic2_data_1.png}}\\
%   \subfloat[]{\label{fig:baselinedata4}\includegraphics[width=0.33\textwidth]{../img/pets2009_data_1.png}} 
%   \subfloat[]{\label{fig:baselinedata5}\includegraphics[width=0.33\textwidth]{../img/pets2009_data_2.png}} 
%   \subfloat[]{\label{fig:baselinedata6}\includegraphics[width=0.33\textwidth]{../img/pets2009_data_3.png}}
%   \caption{Baseline extracted features from the (a.) PETS2001 Dataset, (b.) Ants Dataset, (c.) Traffic Dataset, and PETS2009/2010 benchmark dataset, frames (a.) 1-200, (b.) 201-400, (c.) 401-600. Time is shown on the vertical axis. Marker color corresponds with spatial coordinates and is shown for ease of viewing.}
%   \label{fig:baselinedata}
% \end{figure}

Inference performed on the time dependent Dirichlet process mixture model (Section~\ref{sec:inference}) carries out clustering of the data, which isolates each worm-like data cloud and allows each object to be localized and tracked.






\subsection{Dirichlet Process Mixtures}

Dirichlet process mixtures fall under the heading of Bayesian nonparametric (BNP) models. These models have been widely used over the past decade to perform nonparametric density estimation and cluster analysis. They are, in particular, useful for estimating the number of latent classes (clusters) in mixture models. In this work, estimating the number of clusters in the data generated via extraction is shown in the following sections to be equivalent to estimating the number of distinct objects in a video. Consequentially, a model that is able to perform robust nonparametric estimation of the clusters in an extraction dataset may be applied to perform unsupervised object detection and tracking.

The following sections are intended to provide a solid introduction to DPMs without a major foray into the theoretical aspects of the model that require a more rigorus technical setup. We give background on finite mixture models, Bayesian finite mixture models, and Dirichlet processes, which allows us to subsequently describe DPMs in context. 


\subsubsection{Finite Mixture Model}

A finite mixture model can be thought of as a probability distribution for an observation $x_{i}$ formulated as a linear combination of K mixture components (also called `clusters'), where the coefficients of the linear combination sum to one; each mixture component, in turn, is a probability distribution for $x_{i}$, given some parametric form. The finite mixture model can be written as
\begin{equation}
P(x) = \sum_{k=1}^{K} P(c_{i} = k)P(x|\theta_{k})
\end{equation}
where $c_{i} \in \{ 1, \ldots, K \}$ denotes the assignment of $x_{i}$ to a given mixture (which allows the coefficients of the linear combination to be written as $P(c_{i} = k)$), and $\theta_{k}$ denotes the parametric form of the $k^{\text{th}}$ mixture component. We also define $p_{k} = P(c_{i} = k)$ for $k \in \{ 1, \ldots, K \} $. We can therefore write this model generatively as
\begin{align}
\begin{split}
	c_{i}|p_{1}, \ldots, p_{K}  &\sim  \text{Discrete}(p_{1}, \ldots, p_{K}) \\
	x_{i}|c_{i}, \theta_{c_{i}}  &\sim  F(\theta_{c_{i}})
\end{split}
\end{align}
where the $x_{i}$ (for $i \in \{ 1, \ldots, N \}$) are observations, the $c_{i}$ are the mixture component assignments associated with each observation, and the $\theta_{c_{i}}$ are parameters defining the $c_{i}^{\text{th}}$ mixture component (i.e. the distribution to be mixed, $F(\theta_{c_{i}})$).



\subsubsection{Bayesian (Finite) Mixture Model}


The finite mixture model of the previous section can be extended to a Bayesian mixture model by viewing the distribution parameters that were previously point values, $\theta_{c_{i}}$ (the mixture component parameters) and $p_{1}, \ldots, p_{K}$ (the mixture component assignment weights), as random variables and providing each with a prior distribution. In this case, the prior distribution $\mathbb{G}_{0}$ is placed on the mixture component parameters, and the prior distribution $\text{Dir}(\alpha/K, \ldots, \alpha/K)$ is placed on the K mixture component assignment weights. The resulting Bayesian mixture model can be formulated in a generative representation as
\begin{align}
\begin{split}
\label{bayesian_mixture_model}
	p_{1}, \ldots, p_{K}  &\sim  \text{Dir}(\alpha/K, \ldots, \alpha/K)\\
	c_{i}|p_{1}, \ldots, p_{K}  &\sim  \text{Discrete}(p_{1}, \ldots, p_{K}) \\
	\theta_{c_{i}}  &\sim  \mathbb{G}_{0} \\
	x_{i}|c_{i}, \theta_{c_{i}}  &\sim  F(\theta_{c_{i}})
\end{split}
\end{align}
where the $x_{i}$ (for $i \in \{ 1, \ldots, N \}$) are observations, the $c_{i}$ are the mixture component assignments associated with each observation, the $\theta_{c_{i}}$ are parameters defining the $c_{i}^{\text{th}}$ mixture component (i.e. the distribution to be mixed, $F(\theta_{c_{i}})$), the $\theta_{c_{i}}$ are drawn from the prior distribution $\mathbb{G}_{0}$, and $p_{1}, \ldots, p_{K}$ are drawn from a Dirichlet prior parameterized by $\alpha/K, \ldots, \alpha/K$.




\subsubsection{Dirichlet Process}

% Define Dirichlet distribution first? Each draw from a K-dimensional Dirichlet lies on the K-simplex (i.e. is a vector of K values which sum to 1) and can therefore be thought of as a K-dimensional discrete distribution.

The Dirichlet process (DP), first introduced by \cite{ferguson_1973} in 1973, may be intuitively viewed as a probability distribution over discrete probability distributions. Accordingly, draws from a DP are probability mass functions (PMFs). A DP is parameterized by a base distribution $\mathbb{G}_{0}$, which is a probability distribution over a set $\Theta$, and a concentration parameter $\alpha \in \mathbb{R}_{+}$. We say that $G$ is a random PMF distributed according to a DP, written $G \sim \text{DP}(\alpha, \mathbb{G}_{0})$, if the following holds for all finite partitions $A_{1}, \ldots, A_{p}$ of $\Theta$:
\begin{equation}
(G(A_{1}), \ldots, G(A_{p})) \sim \text{Dir}(\alpha \mathbb{G}_{0}(A_{1}), \ldots, \alpha \mathbb{G}_{0}(A_{p}))
\end{equation}
Where `Dir' denotes a Dirichlet distribution. The parameters $\mathbb{G}_{0}$ and $\alpha$ may be intuitively viewed as the mean and precision of the DP. This is due to the fact that if the base distribution $\mathbb{G}_{0}$ is a distribution over $\Theta$, $A \subset \Theta$, and $G \sim \text{DP}(\alpha, \mathbb{G}_{0})$, then the following holds:
\begin{equation}
\mathbb{E}[G(A)] = \mathbb{G}_{0}(A)
\end{equation}
\begin{equation}
\text{Var}[G(A)] = \mathbb{G}_{0}(A) (1 - \mathbb{G}_{0}(A)) / (\alpha + 1)
\end{equation}
Hence, the expectation of $G(A)$ is $\mathbb{G}_{0}$, the variance of $G(A) \rightarrow 0$ as $\alpha \rightarrow \infty$, and $G$ converges pointwise to $\mathbb{G}_{0}$ when $\alpha$ is unbounded.



\subsubsection{Dirichlet Process (Infinite) Mixture Model}

A Dirichlet process mixture model, also refered to as an infinite mixture model, is an extension of the Bayesian mixture model described previously. 
When using a DP as a prior in a Bayesian mixture model, $\Theta$ can be viewed as the set of parameters of the component mixture distributions. 	A DPM is formulated by allowing the prior distribution over component weights in a standard mixture model to be distributed according to a DP \willie{need to make sure am saying this right}; this allows for modeling data where the true number of latent mixture components is unknown and arbitrarily large by assuming an infinite number of components, of which only a finite amount are expressed by the data. In particular, the DPM can be defined generatively as
\begin{align}
\begin{split}
	\mathbb{G} | \alpha, \mathbb{G}_{0}  &\sim  \text{DP}(\alpha, \mathbb{G}_{0}) \\
	\phi_{i} | \mathbb{G}  &\sim  \mathbb{G} \\
	x_{i}|\phi_{i} &\sim F(\phi_{i})
\end{split}
\end{align}
where the $x_{i}$ (for $i \in \{ 1, \ldots, N \}$) are observations, the $\phi_{i}$ are parameters defining the mixture component from which the $i_{th}$ observation is drawn (i.e. the distribution to be mixed, $F(\phi_{i})$), and the $\phi_{i}$ are drawn from $\mathbb{G}$, which is in turn drawn from a DP with base distribution $\mathbb{G}_{0}$ and parameter $\alpha$. See \cite{gasthaus_2008} and \cite{gasthaus_thesis} for more details on this formulation. Note the difference between the indexing of the clusters in this model and the indexing in the previous two models. This formulation can be shown to be equivalent to the Bayesian mixture model defined in \eqref{bayesian_mixture_model}, when K is taken to be unbounded. This is the origin for references to this model as the infinite mixture model.

An alternative formultion of the DPM is known as the Chinese resturant process (CRP) sampling representation. If we let $K$ be the number of distinct mixture components in the above model, we can write the distinct mixture components as $\theta_{1}, \ldots, \theta_{K}$. Let $c_{1}, \ldots, c_{N}$ (where $c_{i} \in \{1, \ldots, K \}$) be class assignment variables that indicate the cluster to which observation $x_{i}$ is assigned. This allows the DPM to be represented by the CRP, a discrete-time stochastic process that defines a partition of the set $\{ x_{1}, \ldots, x_{N} \}$ (via the elements' assignments $c_{1}, \ldots, c_{N}$). The CRP allows samples to be drawn from the conditional distribution of the indictor variables $c_{i}$, and can be expressed as
\begin{align}
\begin{split}
\label{crp_rep}
	P(c_{i} = c_{j} \text{  for some  } j<i) &= \frac{m_{k}}{i-1+\alpha}\\
	P(c_{i} \neq c_{j} \text{  for all  } j<i) &= \frac{\alpha}{i-1+\alpha}
\end{split}
\end{align}
where $m_{k}$ is the cardinality of the set $\{ c_{j} | (c_{j}=c_{i}=k)  \wedge  (j < i) \}$ and $\alpha$ is the parameter of the DP prior on $\mathbb{G}$.





\subsection{Dependent Dirichlet Process Mixtures}

The goal of dependent Dirichlet process mixtures (DDPMs) is to allow modeling of data that is not independent and identically distributed (i.i.d) but instead has some underlying dependencies. For example, data generated during extraction procedures from videos have some associated temporal structure, since tracked objects display time dependent characteristics (in the features that denote position, as well as those which denote other appearance characteristics).

To account for the dependent behavior of data, research has been conducted on developing models involving a sequence of DPMs, where components of the mixtures are dependent upon (or may be considered `tied to') corresponding components at neighboring positions in the sequence. For example, if the data shows temporal dependence, the goal might be to create a sequence of DPMs, one for each time-step, where the components of the mixture at each step are dependent upon corresponding components in the both the following and previous time steps \willie{cite some of these}.

More rigorously, we take the definition of a DDPM to be a stochastic process defined on the space of probability distributions over a domain, which are indexed by time, space, or a selection of other covariates in such a way that the marginal distribution at any point in the domain follows a Dirichlet process (adapted from definitions found in \cite{gasthaus_thesis} and \cite{griffin2006order}). Hence, a time-dependent DDPM is a model which remains a Dirichlet process, marginally, at each time step, yet allows cluster parameters at a given time step to vary from (and remain dependent upon) the parameters in neighboring time steps.



\subsubsection{Generalized Polya Urn Dependent Dirichlet Process Mixture}
\label{sec:gpudpm}

Data gained by performing extraction on videos containing objects is significantly time-dependent. Futhermore, the clusters of data, each representing a video object, might change in number as time progresses (which is to say, clusters may get `born' and may `die' at some intermediate time step), since objects can enter and exit a scene. To handle these challenges, the specific DDPM chosen to model extraction data in this work is known as the Generalized Polya Urn Dependent Dirichlet Process Mixture (GPUDDPM), introduced by \cite{caron_2007} in 2006.

The GPUDDPM, when applied to data over T discrete time steps, can be viewed as a sequence of DPMs (one for each $t \in \{1, \ldots, T \}$), which are linked together by dependencies between cluster parameters in neighboring time steps. More specifically, the parameters at a given time step are distributed as a function of the parameters in the previous time step, and the distribution and number of distinct cluster assignments at a given time step are distributed according to all previous cluster assignments and a deletion procedure, described below.

A transition kernel $P(\phi_{k}^{t} | \phi_{k}^{t-1})$ specifies how mixture component parameters in time step $t$ are dependent upon associated mixture component parameters in time step $t-1$. One caveat is that each mixture component must be drawn independently from $\mathbb{G}_{0}$ (the base distribution of the DP, which acts as a prior distribution for the cluster parameters) which we achieve by making $\mathbb{G}_{0}$ the invariant distribution of $P(\phi_{k}^{t} | \phi_{k}^{t-1})$ (note that the transition kernel is a markov chain). 

Recall that the distribution over the cluster assignments $c_{i}$ (one for each observation $i$) is a function of the cluster sizes (see \eqref{crp_rep}). Hence, to account for varying numbers of clusters--and in particular, to allow clusters to diminish in size (i.e. reduce the number of assigned observations) and even die--there is a deletion procedure (described in \cite{caron_2007}) by which observations are considered ``removed'' from their assigned clusters at a given time step and value of $m_{k}$ (defined in \eqref{crp_rep} to be the size of cluster $k$) is reduced.

The deletion procedure operates in the following way: at each time step, all previous assignments (that have not yet been deleted) are independently considered for deletion. Specifically, each (remaining) assignment is removed from its cluster with probability $\rho$. We can consider the size $m_{k,t}$ of cluster $k$ at time $t$ to be dependent upon the cluster size at time $t-1$ via a deletion step. It can be shown that performing deletion in this way is equivalent to, for each cluster $k$, drawing $r \sim \text{Binomial}(m_{k,t-1}, \rho)$, and reducing the cluster size $m_{k,t}$ by $r$. This can be written as 
\begin{equation}
\label{del_step}
m_{k,t} | m_{k,t-1}   \sim   m_{k,t-1}-\text{Binomial}(m_{k,t-1}, \rho)
\end{equation}
with deletion parameter $\rho$. This process adheres to what \cite{caron_2007} calls a `uniform deletion strategy' over all the observations' assignments (since assignments to each cluster have equal probability of being deleted), though a more complex deletion strategy, dependent upon cluster size, can also be implemented \cite{caron_2007}. For a given time $t$ and cluster $k$, the size $m_{k,t}$ is dependent upon the cluster size at the previous time step (via \eqref{del_step}) and on the assignments $c_{i}$ for observations at time $t$. We refer to this distribution over $m_{k,t} | m_{k,t-1}, c_{i}, \rho$ as DEL. Using the cluster size and observation deletion terms introduced above, we can define the GPUDDPM generatively as, for each time step $t \in \{1, \ldots, T\}$,  % and observation $i \in \{ 1, \ldots, N_{t} \}$ at $t$,
\begin{align}
\begin{split}
\bold{m}_{t} | \bold{m}_{t-1}, \rho  &\sim \text{DEL}(\bold{m}_{t-1}, \rho) \\
c_{i} | \bold{m}_{t}, \alpha  &\sim  \text{CRP}(\bold{m}_{t}, \alpha) \\
\theta_{c_{i}, t} | \theta_{c_{i}, t-1}   &\sim
\begin{cases}
	P(\theta_{c_{i}, t} | \theta_{c_{i}, t-1}) \\
	\mathbb{G}_{0}
\end{cases} \\
\bold{x}_{i} | c_{i}, \theta_{c_{i}, t} &\sim F(\theta_{c_{i}, t})
\end{split}
\end{align}
where $\text{CRP}(\cdot, \alpha)$ is given in \eqref{crp_rep}. The graphical model corresponding with this formulation is shown in Figure \ref{fig:gpuddpm_gm_1}. The above formulation holds if there is exactly one observation at each time step. Often, the data extracted from objects in videos consists of multiple observations at each time step, each with an assignment variable ($c_{i}$ for $i \in \{ 1, \ldots, N_{t} \}$, where $N_{t}$ is the number of observations at time step $t$). If we allow for multiple observations $i \in \{ 1, \ldots, N_{t} \}$ at each time $t$, this formulation becomes,
\begin{align}
\begin{split}
\bold{m}_{t}^{1} | \bold{m}_{t-1}^{N_{t-1}}, \rho  &\sim \text{DEL}(\bold{m}_{t-1}^{N_{t-1}}, \rho) \\
c_{i} | \bold{m}_{t}^{i-1}, \alpha  &\sim  \text{CRP}(\bold{m}_{t}^{i-1}, \alpha) \\
\theta_{c_{i}, t} | \theta_{c_{i}, t-1}   &\sim
\begin{cases}
	P(\theta_{c_{i}, t} | \theta_{c_{i}, t-1}) \\
	\mathbb{G}_{0}
\end{cases} \\
\bold{x}_{i} | c_{i}, \theta_{c_{i}, t} &\sim F(\theta_{c_{i}, t})
\end{split}
\end{align}
where $\bold{m}_{t}^{i}$ represents the $i^{\text{th}}$ observation at time $t$. 

\begin{figure}[h]
        \center{\includegraphics[width=90mm]{../img/gpuddp_gm_1.pdf}}
        \caption{\label{fig:gpuddpm_gm_1} Graphical Model of the Generalized Poly Urn Dependent Dirichlet Process Mixture}
\end{figure}





\subsubsection{Object Characteristics Modeled}

We would like make explicit the connection between certain elements of the GPUDDPM model and the characteristics of objects in vidoes that these elements represent.

First, the appearances of objects are moded by what we term the `likelihood and object appearance model', which we denoted by $F$. In the above modeling formulation, $F$ is the probability distribution for an observation, given that it is assigned to specified cluster (i.e. given that it is associated with a given object in a video). The likelihood and object appearance model is therefore
\begin{equation}
F(\theta_{c_{i}}) = P(x_{i}|\theta_{c_{i}})
\end{equation}
where $x_{i}$ is an observation, $c_{i}$ is its associated assignment, $\theta_{c_{i}}$ specifies the parameters of the parametric form of the $c_{i}^{th}$ cluster. The specific form of this model is dependent upon the observations $x_{i}$; recall that the observations, which as a baseline include spatial and temporal features, could include an arbitrary amount of additional features which need to be incorporated into this appearance model. A specific formulation of the appearance model can be seen in the proceeding section.

Object behavior is modeled by the dependencies between corresponding clusters at adjacent time steps. In particular, this relationship is captured by the transition kernel, $P(\theta_{k}^{t} | \theta_{k}^{t-1})$ (which captures the dependence of cluster $k$ at time $t$ on the same cluster at time $t-1$), described previously. The motion behavior of an object, i.e. the specific dependence of the parameters of cluster $k$ at a time $t$ on its parameters at time $t-1$, is therefore
\begin{equation}
Tr(\theta_{k, t} | \theta_{k, t-1}) = P(\theta_{k, t} | \theta_{k, t-1})
\end{equation}

Additionally, the base distribution $\mathbb{G}_{0}$ of the DP acts as a prior on the cluster parameters. This allows one to place a prior probability over appearances of objects. It is important that this prior is not chosen in such a specific manner that it limits this method from performing unsupervised detection of arbitrary objects.







\section{Model Specification}
\label{sec:modelspec}

In the following sections, a particular extraction method for generating data from videos and a specific formulation of the model described in Section~\ref{sec:modeloverview} are given. Inference schemes on this model, which when carried out allow for unsupervised detection and tracking of multiple arbitrary objects, are also described.



\subsection{Extraction}
\label{sec:modelspec_extraction}

We desire an extraction procedure that is as unsophisticated as possible, both to gauge the robustness of this method on potentially noisy extraction data and to ensure that the procedure is applicable to a wide range of videos. For these reasons we choose the method of frame-differencing for all extraction performed in this paper, which involves recording the positions of pixels (or pixel groups) that have exhibited differences in intensity or value in succesive frames beyond a given threshold. In particular, if we let $I_{t}$ be the image difference obtained by subtracting the value of the $(i,j)^{th}$ pixel in video frame $t$ from the value of the $(i,j)^{th}$ pixel in video frame $t+1$, we can define extraction on frame $t$ to be the process that returns the dataset
\begin{equation}
	\Omega_{t} = \{ (i,j) | I_{t}(i,j) > \phi \}
\end{equation}
where $i$ and $j$ respectively denote the two spatial positions of a pixel, and $\phi$ is a given pixel value threshold. Frame differencing on each frame $t =\{1, \ldots, T \}$ yields the dataset
\begin{equation}
	\Omega = \bigcup_{t=1}^{T} \Omega_{t} = \{ (i,j,t) | I_{t}(i,j) > \phi \}
\end{equation}
which is equivalent to data with the baseline features described in Section~\ref{sec:data}.

Frame differencing is unsophisticated, computationally inexpensive, able to be applied to a wide range of static, single-camera videos (note that videos used in the experiments described in the following sections were chosen to be static; moving-camera videos should be used in conjunction with applicable extraction methods that allow for camera movement). We have found that, when correctly implemented, frame differencing is sufficiently general to extract the desired worm-like data clouds from a variety of videos containing multiple moving objects. A few examples of this extraction procedure on pairs of consecutive video frames can be found in Figure \ref{fig:img_and_framediff}.

% \begin{figure}
% \centering{
% \includegraphics[width=45mm]{../img/antpic2.png}}
% %\hspace{2mm}
% {\includegraphics[width=60mm]{../img/frame_diff_ants_1.png}}
% \caption{(a.) Frame of a video containing five ants. (b.) Frame differencing extraction results from the same frame \willie{this is just a demo of the type of image i will have here... these aren't actually from the same frame, and also formatting is screwed up)}}
% \label{test}
% \end{figure}

\begin{figure}
  \centering
  \subfloat[]{\label{fig:ants_img}\includegraphics[width=0.335\textwidth]{../img/usbh_frame_723.pdf}} 
  \subfloat[]{\label{fig:ants_img2}\includegraphics[width=0.335\textwidth]{../img/usbh_frame_724.pdf}}
  \subfloat[]{\label{fig:ants_img_framediff}\includegraphics[width=0.323\textwidth]{../img/usbh_framediff.png}}\\
  \subfloat[]{\label{fig:traffic2_img}\includegraphics[width=0.328\textwidth]{../img/traffic2_frame1.png}} 
  \subfloat[]{\label{fig:traffic2_img2}\includegraphics[width=0.328\textwidth]{../img/traffic2_frame2.png}}
  \subfloat[]{\label{fig:traffic2_img_framediff}\includegraphics[width=0.3273\textwidth]{../img/traffic2_framediff.png}}\\
  \subfloat[]{\label{fig:pets2009_img}\includegraphics[width=0.333\textwidth]{../img/pets2009_frame1.png}} 
  \subfloat[]{\label{fig:pets2009_img2}\includegraphics[width=0.333\textwidth]{../img/pets2009_frame2.png}}
  \subfloat[]{\label{fig:pets2009_img_framediff}\includegraphics[width=0.333\textwidth]{../img/pets2009_framediff.png}}\\
  \subfloat[]{\label{fig:longimg}\includegraphics[width=1.05\textwidth]{../img/longimg_6.pdf}} 
  \caption{Pairs of consecutive frames and the results produced by taking the pixel-wise difference between these frames. \willie{Want to add colorbar that explains how color represents frame number red = frame 1, blue = frame 100}}
  \label{fig:img_and_framediff}
\end{figure}

We furthermore wish to capture the color (or grayscale) value of each pixel and incorporate this color information into our model. For each pixel $x = (i, j, t) \in \Omega$ (defined above), we specify a square, $L$ pixels in length, centered on $(i, j)$, that selects a set of pixels surrounding $\bold{x}$ in frame $t$. We also specify a scalar color value that is able to be computed for each pixel; this value could, for example, be some function of the r-g-b or h-s-v value of a pixel. The color value of each of the selected pixels surrounding $\bold{x}$ is recorded. Afterwards, the set of possible color values (ie the range of color values to which a pixel may be assigned) is partitioned into $V$ bins, and the number of pixels with a color value lying in each of the bins yields a $V$ dimensional vector of `color counts'. We will refer to this extraction technique as `color counting'.

\willie{Show example color-count vector (histogram) for different objects in an image. give more-formal definition like for frame differencing above.}







\subsection{Model Implementation in Experiments}

The following sections provide details on the data and specific model formulation (determined from the general model formulation of Section~\ref{sec:modeloverview}) involved in the experiments described in Section~\ref{sec:experiments}.


\subsubsection{Data}

The frame differencing and color counting extraction outlined in Section~\ref{sec:modelspec_extraction} yields a set of data $\bold{X} \subset \mathbb{R}^{2} \times \mathbb{Z}_{+}^{V} \times \mathbb{Z}_{+}$, where each element $\bold{x} \in \bold{X}$ can be written
\begin{equation}
\bold{x} = ( \bold{x}^{s}, \bold{x}^{c}, t ) = ( x^{s_{1}}, x^{s_{2}}, x^{c_{1}}, \ldots, x^{c_{V}}, t )
\end{equation}
where $\bold{x}^{s} \in \mathbb{R}^{2}$ is the two dimensional vector of spatial coordinates,  $\bold{x}^{c} \in \mathbb{Z}_{+}^{V}$ is the $V$ discrete dimensional vector of color counts, and $t \in \mathbb{Z}_{+}$ is the discrete time index. 

In the following experiments, the hue component of a pixel's h-s-v value is recorded from each pixel surrounding a given $\bold{x}^{s}$ in the manner described in Section~\ref{sec:modelspec_extraction} (where we choose $L=5$). Additionally, the set of possible hue values is partitioned into 10 bins, and the number of pixels with a hue value lying in each of the 10 bins is recorded to yield the vector of `color counts' $\bold{x}^{c} = x^{c_{1}}, \ldots, x^{c_{10}}$. Hue is chosen to represent object color since it has been demonstrated in previous work as a simple representation of object appearance that allows for distinct objects to be well differentiated (\willie{cite mckenna, other papers here}).



\subsubsection{Likelihood and Object Appearance Model}

At a given time $t$, we model each $\bold{x} \in \bold{X}$ as a draw from the product of a multivariate normal and multinomial distribution
\begin{equation}
P(\bold{x}|\theta) = \mathcal{N}(\bold{x}^{s} | \boldsymbol{\mu}, \Sigma)  \mathcal{M}n(\bold{x}^{c} | \bold{p})
\end{equation}
where $\theta = \{ \boldsymbol{\mu}, \Sigma, \bold{p} \}$ denote the parameters of a cluster at time $t$, with mean $\boldsymbol{\mu} \in \mathbb{R}^{2}$, covariance matrix $\Sigma \in \mathbb{R}^{2} \times \mathbb{R}^{2}$, and discrete probability vector $\bold{p} = (p_{1}, \ldots, p_{V})$ such that $\sum_{i=1}^{V}p_{i} = 1$. Also, $\mathcal{N}$ denotes the multivariate normal distribution and $\mathcal{M}n$ denotes the multinomial distribution, 

The distribution families assumed to generate each $\bold{x}$ must be justified. Both the multivariate normal and multinomial distributions were chosen because they are sufficiently simple (and well studied) to allow for tractable inference and sufficiently flexible to provide a reasonable approximation to the data gained during extraction. In particular, the multivariate normal distribution over the spatial features $\bold{x}^{s}$ can be thought to represent the shape of each object as an oval; likewise, data generated by each moving object during extraction are often ovular--as noisy extraction procedures cause some smoothing of edges and corners, producing blob-like shapes even when objects are not particularly round--and centered on a given object. Furthermore, this model is justified as the maximum likelihood parameter estimate of a normal distribution corresponds to the least squares fit of data relative to the mean of the distribution. \willie{needs to be said more accurately and explained why this helps.} Modeling the color features $\bold{x}^{c}$ as draws from a multinomial distribution (equivalently, as draws from a product of discrete distributions), is justified since distinct object tend to generate pixels whose hue values are noisy but yield consistent counts in discrete hue bins.



\subsubsection{Base Distribution $\mathbb{G}_{0}$ and Appearance Prior}

$\mathbb{G}_{0}$ denotes the base distribution of the time-dependent Dirichlet process mixture; it also serves as a prior distribution for the parameters $\theta = \{ \boldsymbol{\mu}, \Sigma, \bold{p} \}$ present in the likelihood. We make use of conjugate priors in the base distribution to allow for more efficient computation. Specifically, in the experiments carried out in this paper, a normal-inverse-Wishart prior is placed on the multivariate normal parameters $\{ \boldsymbol{\mu}, \Sigma \}$ , and a Dirichlet prior is placed on the multinomial parameter $ \{  \bold{p}  \} $ (where the normal-inverse-Wishart is a conjugate prior for the multivariate normal component of the likelihood and the Dirichlet is a conjugate prior for the multinomial component). The prior can thus be written
\begin{equation} \label{basedistro}
\mathbb{G}_{0}(\theta) = \mathcal{N}i\mathcal{W}(\boldsymbol{\mu}, \Sigma | \boldsymbol{\mu}_{0}, \kappa_{0}, \nu_{0}, \Lambda_{0})  \mathcal{D}ir( \bold{p} | \bold{q}_{0})
\end{equation}
where $\mathcal{N}i\mathcal{W}$ denotes the normal-inverse-Wishart distribution, $\mathcal{D}ir$ denotes the Dirichlet distribution, and the prior has the hyperparameters $\boldsymbol{\mu}_{0}, \kappa_{0}, \nu_{0}, \Lambda_{0}$ and $\bold{q}_{0}$.  \willie{reference text with good background on NiW and Dir distributions}



\subsubsection{Transition Kernels and Motion Behavior}

We wish to formulate a transition kernel $P(\theta_{t} | \theta_{t-1})$ that provides a reasonable representation of how we expect tracked objects to behave. In the interest of formulating a model with the intention of using it to track arbitrary objects, we do not wish to make many assumptions about the long-term behavior of objects (though transition kernels that incorporate, for example, dynamics could potentially be implemented if one is aware beforehand of an object's behavior).

As described in Section~\ref{sec:gpudpm}, $\mathbb{G}_{0}$ must be the invariant distribution of $P(\theta_{t} | \theta_{t-1})$ in order for the the cluster parameters to remain marginally distributed according to the base distribution. In other words, the transition kernel must satisfy
\begin{equation}
\int \mathbb{G}_{0}(\theta_{t-1})P(\theta_{t} | \theta_{t-1}) d\theta_{t-1} = \mathbb{G}_{0}(\theta_{t})
\end{equation}
for a given cluster $\theta$. One way to achieve this is through the use of auxiliary variables. Auxiliary variables are a set of $M$ variables $\bold{z}_{t} = (z_{t,1}, \ldots, z_{t,M})$ associated with a each cluster $\theta$ at time $t$ that satisfy
\begin{eqnarray}
% P(\theta_{k,t} | \theta_{k,t-1}) = \int P(\theta_{k,t} | \bold{z}_{k,t}) P(\bold{z}_{k,t} | \theta_{k,t-1}) d \bold{z}_{k,t}
P(\theta_{t} | \theta_{t-1}) = \int P(\theta_{t} | \bold{z}_{t}) P(\bold{z}_{t} | \theta_{t-1}) d \bold{z}_{t}  %\\
% P(\theta', \bold{z}_{t}) = p(\bold{z}_{t}|\theta') \mathbb{G}_{0}(\theta')
\end{eqnarray}

In this way, the parameters of a cluster at a given time do not depend directly on their value at the previous time; they are instead dependent upon an intermediate sequence of auxiliary variables chosen to satisfy the above criteria, which allows the cluster parameters at each time step to be marginally distributed according to the base distribution $\mathbb{G}_{0}$.

For each cluster, we introduce $M$ auxiliary variables $z_{t, 1}, \ldots, z_{t, M}$ at time $t$ that are each drawn from the product of a multivariate normal and multinomial when conditioned on the associated cluster parameters $\theta_{t} = \{ \boldsymbol{\mu}_{t}, \Sigma_{t}, \bold{p}_{t} \}$:
\begin{equation} \label{transker1}
z_{t, m} | \boldsymbol{\mu}_{t}, \Sigma_{t}, \bold{p}_{t}  \sim  \mathcal{N}(\boldsymbol{\mu}_{t}, \Sigma_{t}) \mathcal{M}n(\bold{p}_{t})   \hspace{15pt}   
\forall m \in \{ 1, \ldots, M \}
\end{equation}
To satisfy the above criteria for auxiliary variables, at each time $t$ we specify the dependencies of a given cluster on its associated set of auxiliary variables by
\begin{equation} \label{transker2}
\boldsymbol{\mu}_{t}, \Sigma_{t}, \bold{p}_{t} | \bold{z}_t  \sim  \mathcal{N}i\mathcal{W}(\boldsymbol{\mu}_{M}, \kappa_{M}, \nu_{M}, \Lambda_{M})  \mathcal{D}ir(\bold{q}_{M})
\end{equation}
where $\boldsymbol{\mu}_{M}, \kappa_{M}, \nu_{M}, \Lambda_{M},$ and $\bold{q}_{M}$ are parameters given by
\willie{these are given in next section, need to consider if I actually want to repeat them here}



\subsection{Recap of Parameters}
\label{sec:parameter_recap}

The multivariate normal-multinomial GPUDDPM implemented in this study has a number of parameters, which can be tuned to increase the efficacy of object detection and tracking. Object apperance parameters include\\\\
\begin{center}
\begin{tabular}[c]{l p{10cm}}
$\boldsymbol{\mu}_{0}$  &  The mean prior. In experiments performed in Section~\ref{sec:experiments} the data was recentered to the origin, and this parameter was set to $(0,0)$ \\
$\kappa_{0}$  &  scale factor of the variance of the prior on the mean\\
$\nu_{0}$  &  scale factor of the variance of the prior on the covariance\\
$\Lambda_{0}$  &  shape factor of the prior on the covariance\\
$q_{0}$  &  scale factor of the prior on the multinomial counts
\end{tabular}
\end{center} \vspace{5mm}
The following parameter dictates characteristics involving the movement of objects
\begin{center}
\begin{tabular}[c]{l p{10cm}}
$M$  &  Number of auxiliary variables. A large number will produce lower variation in position (better for slower objects) and a low number will produce higher variation in position.
\end{tabular}
\end{center} \vspace{5mm}
Additionally, one can tune the model's tendency to detect new objects and maintain the existence of these objects (or the rate at which objects become inactive or leave the video) with the following parameters
\begin{center}
\begin{tabular}[c]{l p{10cm}}
$\alpha$  &  The granularity parameter for the Dirichlet Process. A higher value will increase the tendency for new objects to be detected.\\
$\rho$  &  The deletion parameter. A higher value will give objects an increased tendency to die off.
\end{tabular}
\end{center}
\willie{Discuss choosing parameters}


\section{Inference}
\label{sec:inference}
Algorithms that infer latent model parameters in a Bayesian manner given the data gained during extraction provide detection and tracking results. By formulating the detection and tracking task in terms of a generative model, a variety of methods previously developed for statistical inference can be readily applied; we separate presentations of model and inference so that multiple inference methods, each with its own benefits in terms of precision and speed, can be formulated and applied independently to gain detection and tracking results. This section provides background on the two inference procedures implemented in this study, used to perform Bayesian inference on the model described in Section~\ref{sec:modelspec}. The first is a type of Markov Chain Monte Carlo (MCMC) batch inference, which uses Gibbs sampling to generate samples from a posterior distribution over a model, and the second is a type of Sequential Monte Carlo (SMC) inference, also known as a particle filter, which also generates samples from a posterior information, but in this case in a sequential manner that is allowed due to the time-dependent nature of the model.


\subsection{MCMC Batch Inference}
\label{sec:MCMC}

This section details a Markov Chain Monte Carlo (MCMC) sampler used to perform inference, which provides a method to carry out unsupervised detection and tracking. It operates by iteratively sampling from conditional distributions of elements in the joint probability distribution--i.e. it implements Gibbs sampling--in order to gain approximate samples from the posterior distribution of the cluster parameters given the data. Additionally, this sampler makes use of the Metropolis-Hastings (M-H) algorithm to sample from a few distributions.

The generative model associated with the GPUDDPM is illustrated in Section~\ref{sec:gpudpm}. This representation includes cluster size $m$ (the number of observations assigned to a given cluster), which is standard in DPM constructions. To capture time-dependence, the GPUDDPM allows clusters to `lose assigned observations' in a deletion step, which modifies the sizes $m$ over time. Instead of incorporating cluster size variables $m$ directly, we can formulate an equivalent model which makes use of deletion variables $d$, which represent the time when observations are lost from a cluster . At each time step, cluster sizes can be recontructed from all previous assignment and deletion variables. In particular, we introduce variables $d_{1}, \ldots, d_{N}$ which denote the times at which the assignments of given observations are considered to be removed. With these variables, we can now express the $m_{k}$ value at time $t$ as
\begin{equation}
\label{compute_clust_size}
m_{k,t} = \sum_{t' = 1}^{t} \mathbb{I}[(c_{t'}=k) \wedge (t < d_{t'})]
\end{equation}
where $\mathbb{I}[\cdot]$ is an indicator function that evaluates to 1 if its argument is true, and 0 otherwise. Additionally, for an observation $x_{i}$ at a given time-step $t$, the deletion time $d_{i}$ can be defined to be $d_{i} = t + l_{i}$, where $l_{i}$ is considered the lifespan of an assignment, is distributed geometrically, and can be expressed as
\begin{equation}
\label{del_rho_form}
l_{i} | \rho  \sim  \rho(1 - \rho)^{l_{i}}
\end{equation}
The MCMC sampler described in this section operates in this alternate representation, which we can formulate, for each time step $t \in \{1, \ldots, T\}$ and observation $i \in \{ 1, \ldots, N_{t} \}$ at time $t$, as
\begin{align}
\begin{split}
	 \bold{x}_{i} | c_{i}, \theta_{c_{i},t} &\sim \mathcal{N}(\boldsymbol{\mu}_{c_{i},t}, \Sigma_{c_{i},t})  \mathcal{M}n(\bold{p}_{c_{i},t}) \\
	c_{i} | c_{1}, \ldots, c_{i-1}, d_{1}, \ldots, d_{i}, \alpha  &\sim  \text{CRP}(c_{1}, \ldots, c_{i-1}, d_{1}, \ldots, d_{i}, \alpha) \\
	d_{i} | \rho  &\sim \text{Geo}(\rho) + i + 1\\
	\theta_{k, t} | \theta_{k, t-1}, \boldsymbol{\mu}_{0}, \kappa_{0}, \nu_{0}, \Lambda_{0}  &\sim 
\begin{cases}
	\text{TK} \hspace{12pt} \text{if } k \in \{ 1, \ldots, K_{t-1}  \} \\
	\mathbb{G}_{0}(\boldsymbol{\mu}_{0}, \kappa_{0}, \nu_{0}, \Lambda_{0})  \hspace{15pt} \text{if } k = K_{t-1} + 1
\end{cases}
\end{split}
\end{align}
where TK stands for the transition kernel, given in \eqref{transker1} and \eqref{transker2}, and the base distribution $\mathbb{G}_{0}$ is given in \eqref{basedistro}. This formulation of the GPUDDPM is also used in \cite{gasthaus_thesis} and \cite{caron_2007}. The associated graphical model for this formulation is given in \ref{fig:gpuddpm_gm_2}.
\begin{figure}[h]
        \center{\includegraphics[width=100mm]{../img/gpuddp_gm_3.pdf}}
        \caption{\label{fig:gpuddpm_gm_2} Graphical model of the deletion-variable representation of the GPUDDPM}
\end{figure}


We define the state of the sampler to consist of the assignment variables for all observations, $c_{1}, \ldots, c_{N}$, the deletion variables for all observations, $d_{1}, \dots, d_{N}$, the sequence of cluster parameters for each active cluster $k$, $\theta_{k,1}, \ldots, \theta_{k, T}$, and the $M$ associated auxiliary variables for each cluster $\theta_{k,t}$, $z_{k,t,1}, \ldots, z_{k, t, M}$ (where $N$ denotes the number of observations and $T$ denotes the number of time steps).

The sampler performs Gibbs sampling; it operates by iteratively sampling each member of the state given all other members. In particular, it moves sequentially through each of the $N$ observations, sampling the assignment $c_{i}$ and the deletion variable $d_{i}$ at each step. After the final observation at a given time $t$ step is sampled, the cluster parameters for all active clusters at $t$ are sampled, and a M-H step is employed to sample the $M$ auxiliary variables for each active cluster. The following sections detail how each sample is generated will be explicitely shown in the following sections.



\subsubsection{Sampling Assignment Variables $c_{i}$}
\label{sec:sample_assignments}

A value proportional to the posterior probability can be computed for each possible value that $c_{i}$ may take on; these values allow us to construct a discrete probability distribution that may be sampled from to get a sample from the posterior distribution over assignments. The possible values that the assignment may take on are $c_{i} = k \in \{ 1 , \ldots ,  K_{t}^{'}, k^{*}\}$, where $K_{t}^{'}$ is the number clusters with a non-zero size at any point between observation $i$ and its deletion point $d_{i}$, and $k^{*}$ is a potential new cluster. The probability that $c_{i}$ is assigned to a cluster $k$ can be computed for each $k \in \{ 1 , \ldots ,  K_{t}^{'}, k^{*} \}$, given all other members of the state, by
\begin{equation}
p(c_{i} = k | state) \propto \prod_{i' = i}^{d_{i}} P(c_{i'} | m_{i'-1}, c_{1:i'-1}) \newline \times
\begin{cases}
	P(\bold{x}_{i} | \theta_{k,t})                                        \hspace{12pt} \text{if} \hspace{4pt} k \in \{ 1, \ldots, K_{t}^{'} \} \\
	\int P(\bold{x}_{i} | \theta) \mathbb{G}_{0}(\theta) d\theta    \hspace{12pt} \text{if}  \hspace{4pt}  k = k^{*}
\end{cases}
\end{equation}
\willie{$d_{i}$ is actually a time; product needs to actually go up to last observation at time $d_{i}$} where $t$ is the time of observation $i$, and the distribution under the product $P(c_{i'} | m_{1:K_{t}^{'}, i'-1}, c_{1:i'-1})$ is given by 
\begin{equation}
P(c_{i} = k | m_{1:K_{t}^{'}, i-1}, c_{1:i-1}, \alpha) \propto 
\begin{cases}
m_{k, i-1} \hspace{12pt} \text{if} \hspace{4pt} k \in \{ 1, \ldots, K_{t}^{'} \} \\
\alpha \hspace{12pt} \text{if}  \hspace{4pt}  k = k^{*}
\end{cases}
\end{equation}
which is taken under the situation where $c = k$. If a new cluster is sampled as an assignment, the cluster parameters and auxiliary variables must be initialized for all time steps. \willie{finish writing the few tricky technical stuff here}


\subsubsection{Sampling Cluster Parameters $\theta_{k, t}$}
The conjugacy present in chosen distributions and auxiliary variables in the transition kernel allow for posterior samples of the cluster parameters to be easily gained, and gives us
% \begin{eqnarray}
% P(\theta_{k,t} | state) && = P(\bold{x}_{i} | \theta_{k,t}) P(z_{k,t+1,1:M} | \theta_{k,t})  P(\theta_{k,t} | z_{k,t,1:M}) \\
%  && = \mathcal{N}i\mathcal{W}(\boldsymbol{\mu}, \Sigma | \boldsymbol{\mu}_{0}, k_{0}, v_{0}, \Lambda_{0})  \mathcal{D}ir( \bold{p} | \bold{q}_{0})
% \end{eqnarray}
\begin{align}
\begin{split}
P(\theta_{k,t} | state) & = P(\bold{x}_{i} | \theta_{k,t}) P(z_{k,t+1,1:M} | \theta_{k,t})  P(\theta_{k,t} | z_{k,t,1:M}) \\
 & = \mathcal{N}i\mathcal{W}(\boldsymbol{\mu}_{k,t}, \Sigma_{k,t} | \boldsymbol{\mu}_{N}, k_{N}, v_{N}, \Lambda_{N})  \mathcal{D}ir( \bold{p}_{k,t} | \bold{q}_{N})
\end{split}
\end{align}
Where the parameters in the above distribution are given when the data at time $t$, auxiliary variables $z_{k,t-1,1:M}$ at time $t-1$, and auxiliary variables $z_{k,t,1:M}$ at time $t$, are taken to be the observations in the following Bayesian updates
\begin{eqnarray}
k_{N} &=& k_{0} + N \\
v_{N} &=& v_{0} + N \\
\boldsymbol{\mu}_{N} &=& \frac{k_{0}}{k_{0}+N} \boldsymbol{\mu}_{0}  +  \frac{N}{k_{0}+N} \overline{\bold{x}}^{s}\\
\Lambda_{N} &=& \Lambda_{0} + S_{\bold{x}^{s}}\\
\bold{q}_{N} &=& \bold{q}_{0} + \sum_{i=1}^{N} \bold{x}^{c}
\end{eqnarray}
where $N$ is the number of observations, $\{ \boldsymbol{\mu}_{0}, k_{0}, v_{0}, \Lambda_{N} \}$ are the $\mathcal{N}i\mathcal{W}$ prior parameters, $\bold{q}_{0}$ is the $\mathcal{D}ir$ prior parameter, $\bold{x}^{s}$ and $\bold{x}^{c}$ respectively denote the spatial and color features of the observations, and $\overline{\bold{x}}$ and $S_{\bold{x}}$ respectively denote the sample mean and sample covariance for a set of observations $\bold{x}$, defined to be
\begin{eqnarray}
\overline{\bold{x}}  &=&  \left( \sum_{i=1}^{N} \bold{x}_{i} \right) / N\\
S_{\bold{x}}  &=&  \sum_{i=1}^{N} (\bold{x}_{i} - \overline{\bold{x}}) (\bold{x}_{i} - \overline{\bold{x}})^{T}
\end{eqnarray}

% \subsubsection{Metropolis-Hastings Algorithm}

\subsubsection{Sampling Auxiliary Variables $z_{k,t,m}$}
The posterior distribution of each of the auxiliary variables $z_{k,t,m}$ can be written as
\begin{equation}
P(z_{k,t,m} | state) \propto  P(z_{k,t,m} | \theta_{k,t-1}) P(\theta_{k,t} | z_{k,t,1:M})
\end{equation}
This distribution does not allow for samples to be easily drawn and thus requires a more sophisticated MCMC technique. In particular, the Metropolis-Hastings (M-H) algorithm is used to gain samples from the posterior distribution for the auxiliary variables; it involves constructing a markov chain with the sought posterior as its stationary distribution, and sampling sequentially from this constructed chain to get an approximate posterior sample. The M-H algorithm is outlined in Algorithm~\ref{alg:MH}. \willie{am i citing wrong?}

\begin{algorithm}[h]
\label{alg:MH}
\caption{Metropolis Hastings Algorithm}
\begin{algorithmic}[1]
\STATE $f(x)$ is the desired stationary PDF
\STATE $P(x_{i} | x_{i-1})$ is the proposal pdf
\STATE Initialize $x_{0}$
\FOR{$i = 1 : burn\hspace{1pt}in + N$}
\STATE Draw $x^{*} \sim P(x_{i} | x_{i-1})$
\STATE Draw $\alpha \sim U(0,1)$
\STATE $r_{accept} \leftarrow \frac{f(x^{*})P(x_{i-1} | x^{*})} {f(x_{i-1})P(x^{*} | x_{i-1})} $
\IF{$\alpha < min \{ 1, r_{accept} \} $}
\STATE $x_{i} \leftarrow x^{*}$
\ELSE
\STATE $x_{i} \leftarrow x_{i-1}$
\ENDIF
\ENDFOR
\STATE Discard samples $x_{1}, \ldots, x_{burn\hspace{1pt}in}$
\STATE \textbf{Output}: Remaining samples $x_{1}, \ldots, x_{N}$
\end{algorithmic}
%\caption{Say thing at end}
\end{algorithm}


The M-H algorithm is used to sample auxiliary variables for each alive cluster. In short, we sample a new value for all $M$ auxiliary variables, denoted $z_{k,t,m}^{*}$, according to the following proposal distribution
\begin{equation}
z_{k,t,m}^{*}  \sim  P(z_{k,t,m} | \theta_{k,t}) = \mathcal{N}(z_{k,t,m}^{s} | \boldsymbol{\mu}_{k,t}, \Sigma_{k,t}) \mathcal{M}n(z_{k,t,m}^{c} | \bold{p}_{k,t})
\end{equation}
compute the standard M-H acceptance ratio, which in this case (due to the proposal distribution choice) simplifies to 
\begin{equation}
r_{accept} = \frac{P(\theta_{k,t-1} | z_{k,t,m}^{*})}{P(\theta_{k,t-1} | z_{k,t,m})}
\end{equation}
and keep this sample with probability $min(1, r_{accept})$.


\subsubsection{Sampling Deletion Variables $d_{i}$}
Computing the posterior distribution (or a value proportional to this distribution) for each possible value that a given deletion variable may take on (in a manner similar to how samples from the posterior distribution of the assignment variables $c_{i}$ are gained in Section~\ref{sec:sample_assignments}) may be computationally expensive due to the large number of potential deletion times. To remedy this, the M-H algorithm may again be used to sample from the posterior, where the prior distribution for the deletion times can be used to propose new samples $d_{i}^{*}$. As the `lifetime' of each assignment can be shown to be geometrically distributed, we can propose a new deletion time sample with
\begin{align}
\begin{split}
l_{i}  &\sim  \text{Geo}(\rho)  \\
d_{i}^{*}  &= l_{i} + t + 1
\end{split}
\end{align}
and accept or reject this sample using the standard M-H acceptance ratio \willie{write computation of ratio}.



\subsubsection{Initializing the Sampler}
Before the samples can be iteratively drawn from each element of the state given all other elements, the state must be initialized. While the state of the sampler can theoretically be initialized at any value, beginning at a value closer to the maximum joint probability (over the state) allows for quicker inference.



\subsection{SMC Inference}
\label{sec:SMC}

This section details a Sequential Monte Carlo (SMC) sampler--also known as a particle filter--used to perform inference, which provides another method to carry out unsupervised detection and tracking. An overview of SMC inference is given in Algorithm~\ref{alg:SMC}. \willie{general overview here: at each time step, perform sampling, and then show algorithm figure / thing}

\begin{algorithm}
\caption{Sequential Monte Carlo Inference for the GPUDDPM}
\label{alg:SMC}
\begin{algorithmic}[1]
\FOR{$t = 1 : T$}
\FOR{$l = 1 : L$}
\STATE $K_{t}^{(l)}  \leftarrow K_{t-1}^{(l)}$
\FOR{$i = 1 : N_{t}$}
\STATE Sample assigments from previous time sizes and cluster parameters $\theta_{k,t-1}$
\STATE Draw $c_{i} \sim P(c_{i} | m_{1:K_{t-1}, i-1}, \theta_{1:K_{t-1}, t-1} )$
\IF{$c_{i} = K_{t} + 1$}
\STATE Draw a new cluster $\theta_{K_{t}+1,t} \sim \mathbb{G}_{0}(\text{priors})$   \willie{distro after algo?}
\STATE $K_{t} \leftarrow K_{t} + 1$
\ENDIF
\ENDFOR
\FOR{$s = 1 : S$}
\FOR{$k = 1 : K$}
\STATE Sample new cluster params via aux (given curent assignments and prev clusters)
\ENDFOR
\FOR{$i = 1 : N_{t}$}
\STATE Sample new assignments (given current cluster params)
\STATE Draw $c_{i} \sim P(c_{i} | m_{1:K_{t}, i-1}, \theta_{1:K_{t}, t} )$
\ENDFOR
\STATE Sample new cluster sizes.
\ENDFOR
\ENDFOR
\STATE Only keep some of the $L$ samples.
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Proposal Distribution for Assignments $c_{i}$}
The probability of assignments given current cluster sizes and cluster parameters can be written as
\begin{equation}
P(c_{i} = k) = P(c_{i} | m_{1:K_{t}, i-1}, \theta_{1:K_{t}, t} )
\end{equation}


\subsubsection{Proposal Distribution $q_{1}$}

\begin{equation}
P(\theta_{k,t}) = P(c_{i} | m_{1:K_{t}, i-1}, \theta_{1:K_{t}, t} )
\end{equation}


\subsubsection{Proposal Distribution $q_{2}$}

\begin{equation}
P(\theta_{k,t}) = 	P(c_{i} | m_{1:K_{t}, i-1}, \theta_{1:K_{t}, t} )
\end{equation}



\subsection{Modeling Background Noise}

Noisy extraction procedures may result in erroneous data points that do not correspond to any foreground object. We would like to incorporate these points into our model. To do this, we modify our base distribution $\mathbb{G}_{0}$ to be a two component mixture, where one component is the base distribution and the other is a multivariate normal distribution with a fixed `flat' covarance $\sigma = [100, 0; 0, 100]$ multiplied with an `even' multinomial distribution \willie{need more rigorus terms here} with a fixed parameter $\bold{p} = (1/K, \ldots, 1/K)$.


\subsection{From Inference to Tracking Results}
\label{sec:inference_to_results}
The multivariate normal-multinomial model on which inference is described in this section can be used to gain object detection and tracking results. The nature of object detection and tracking results may vary depending on the application or performance metrics used (see Section~\ref{sec:performance_metrics}). In this study, we desired the centroid position and approximate spatial region for each moving object, for each frame that this object can be seen within the video. We chose to make our results depend completely on the inferred parameters of the multivariate normal distribution of each cluster. Each inferred cluster was taken to be a distinct detected object, and the sequence of means and covariance matrices for a given cluster were used, respectively, to determine the position and spatial region of a given object over a sequence of time steps. In particular, the mean parameter was taken to be the centroid of an object, and a 2-dimensional oval denoting a boundary around the mean that contains a specified percentage of the data (where the specification for this oval is given in Section~\ref{sec:pets2000_2001}) was taken to be the spatial region of the object. Furthermore, as both provided inference methods use sampling, and each sample represents a result, we must choose a way to narrow these down to a single result. We have found that calculating the joint log probability for each sample and choosing the sample that yields the highest value as a result works reasonably well.







\section{Experiments}
\label{sec:experiments}

This section provides details on performance evaluation metrics that have been developed to quantify results in object detection and tracking studies (and adopted in this paper), synthetic video experiments that verify certain aspects of the developed technique, benchmark video experiments that demonstrate the performance of this technique in relation to other methods developed in recent years, and new video experiments to show the capability of this technique to perform unsupervised detection and tracking on a range of object and video types.   


\subsection{Performance Evaluation Metrics}
\label{sec:performance_metrics}

Performance evaluation metrics, which intend to provide a standardized way of quantifying the success of a detection and tracking procedure on a given video, have started to become well studied and consistently used in the past four years. The metrics developed in \cite{kasturi_2008} and used in \cite{ellis_2010, taj_2007, lee_2009} have become well established metrics for evaluating the performance of object detection and tracking in videos and have been adopted by the Video Analysis and Content Extraction (VACE) program and the Classification of Events, Activities, and Relationships (CLEAR) consortium, two large-scale efforts concerned with video tracking and interaction analysis. These metrics are used to quantify the experimental results in this study.

The above metrics are dependent upon ground-truth data specifying the positions of each object in each frame throughout a video sequence. In the experiments described below, we gained the synthetic video ground-truth during construction of the videos (described in Section~\ref{sec:syntheticvideos}), and we used the Video Performance Evaluation Resource (ViPER) ground-truth software \cite{doermann_2000}, an open source tool commonly used in the video tracking community, to author ground-truth data for each of the benchmark datasets.


\subsubsection{Mapping Ground-Truth to Output}
\label{sec:mappingtoground}

The problem of finding a mapping between a video's ground-truth tracks and an algorithm's output tracks is nontrivial, though necesary to solve, in order to compute the performance evaluation metrics used in this study. In short, the typical solution to this problem involves first specifying a performance metric and then choosing the mapping from ground-truth tracks to output tracks which yields the most favorable performance metric value. This process is described in detail by Kasturi et al. \cite{kasturi_2008}; we follow the method outlined in this paper to find an optimal mapping. Similiar to descriptions in \cite{kasturi_2008}, we implement the Hungarian algorithm \cite{munkres_1957} as a polynomial-time ($O(n^{3})$) solution to the problem of optimally mapping two sets of tracks once the similarity between any two tracks given some specified metric is established. Additionally, the method employed in \cite{kasturi_2008} allows erroneous and undetected tracks to be left unmapped, which is both desired and necessary in the case where there is a different number of ground-truth and output tracks. Note that once a mapping from a collection of ground-truth tracks to a collection of result tracks has been established, one can determine which result tracks are false positives (the result tracks to which no ground-truth track is assigned) and which ground-truth tracks are true negatives (the ground-truth tracks that are not assigned to a result track). The numbers of tracks displaying both of these failures are factors in the performance metrics used in this study.

%, where a numerical search algorith--the Hungarian algorithm \cite{munkres_1957}--is used to find an optimal mapping from ground-truth to output tracks without requiring the performance metric value of all possible mapping combinations to be computed (reducing the time complexity from factorial to polynomial time)


% \subsubsection{MODA and MOTA}

% The two metrics used to quantify results in this study are refered to as the Multiple Object Detection Accuracy (MODA) and Multiple Object Tracking Accuracy (MOTA). These metrics, along with the the ground-truth tracks of all objects in the video sequence and the the mapping as defined in the previous section, can be used to evaluate detection and tracking accurancy of the video. For a given video frame $t$, we define MODA as
% \begin{equation}
% MODA = 1 - \frac{m^{t} + e^{t}}{N_{g}^{t}}
% \end{equation}
% where $m^{t}$ is the number of missed object detections (ie true-negative detections), $e^{t}$ is the number of erroneous object detections (ie false positive detections), and $N_{g}^{t}$ is the number of ground truth objects (all at frame $t$).

% For a sequence of video frames $t \in \{ 1, \ldots, T \}$ we define MOTA as
% \begin{equation}
% MOTA = 1 - \frac{\sum_{t=1}^{T} m^{t} + e^{t} + s_{t}}{\sum_{t=1}^{T} N_{g}^{t}}
% \end{equation}
% where $s$ denotes the number of instances at frame $t$ where an object's identity switches from a previous frame (ie where the object's mapping differs between frame $t$ and frame $t-1$),  and the other terms are defined the same as above.



\subsubsection{SFDA and ATA}

The two metrics used to quantify performance in this study are known as the the Sequence Frame Detection Accuracy (SFDA) and the Average Tracking Accuracy (ATA). These metrics were developed during VACE Phase II to provide a single, comprehensive metric to describe detection, and one to describe tracking.

The following terms are used in the definitions of the performance metrics:
\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
\item $G_{i}$ denotes the spatiotemporal region occupied by the $i$th ground-truth object in a video, and $G_{i}^{(t)}$ denotes the region occupied by the $i$th ground-truth object in frame $t$.
\item $D_{i}$ denotes the spatiotemporal region occupied by the $i$th detected object in a video, and $D_{i}^{(t)}$ denotes the region occupied by the $i$th detected object in frame $t$.
\item $N_{G}$ denotes the total number of unique ground-truth objects in a video, and $N_{G}^{(t)}$ denotes the number of unique ground-truth objects present at frame $t$.
\item $N_{D}$ denotes the total number of unique detected objects in a video, and $N_{D}^{(t)}$ denotes the number of unique detected objects present at frame $t$.
\item $N_{\text{frames}}$ denotes the total number of frames in a video, and $N_{\text{frames}}^{(i)}$ denotes the number of frames in which an object $i$ (which can be a ground-truth or detected object, depending on the context) is present in a video.
\item $N_{\text{mapped}}$ denotes the number of mapped ground-truth/detect pairs in a video, and $N_{\text{mapped}}^{(t)}$ denotes the number of mapped ground-truth/detect pairs present at frame $t$.
\end{itemize}

The SFDA metric quantifies the performance of an object detection algorithm as a function of the number of correct detects, false positive detects, missed (true negative) detects, and spatial allignment of detects relative to the ground-truth. The SFDA is calculated by computing the Frame Detection Accuracy at frame $t$ ($\text{FDA}^{(t)}$) for each frame in a video sequence. The FDA provides a measure of the allignment between ground-truth and detected objects in a given frame via the overlap ratio of a ground-truth/detect pair, defined to be the ratio of the intersection of ground-truth and detect regions to the union of ground-truth and detect regions. Formally, we can write
\begin{equation}
\text{FDA}^{(t)} = \frac{\text{Overlap Ratio}}{\left(\frac{N_{G}^{(t)} + N_{D}^{(t)}}{2}\right)}
\end{equation}
where
\begin{equation}
\text{Overlap Ratio} = \sum_{i = 1}^{N_{\text{mapped}}^{(t)}} \frac{\left|G_{i}^{(t)} \cap D_{i}^{(t)}\right|}{\left|G_{i}^{(t)} \cup D_{i}^{(t)}\right|}
\end{equation}
The term $N_{\text{mapped}}^{(t)}$ refers to an optimal mapping between ground-truth and detects at frame $t$ as specified in section \ref{sec:mappingtoground} using the $\text{FDA}^{(t)}$ as the relevant metric. Given the $\text{FDA}^{(t)}$ at each frame, the SFDA can be computed; this metric may be viewed as the average FDA over all frames of a video sequence. We define
\begin{equation}
\text{SFDA} = \frac{\sum_{t=1}^{N_{\text{frames}}} \text{FDA}^{(t)}}{\sum_{t=1}^{N_{\text{frames}}} \exists \left( N_{G}^{(t)} \vee N_{D}^{(t)} \right)}
\end{equation}
where $\exists \left( N_{G}^{(t)} \vee N_{D}^{(t)} \right)$ yields a 1 if either a detected or ground-truth object is present in frame $t$ and a 0 otherwise.

The ATA metric quantifies the performance of an object tracking algorithm as a function of the spatial overlap of a mapped set of sequences of detected object positions to a set of sequences of groundtruth object positions. The ATA is calculated by first computing the Sequence Track Detection Accuracy (STDA), which can be viewed as a tracking performance measure unnormalied in terms of the number of objects. We can write the STDA as
\begin{equation}
\text{STDA} = \sum_{i=1}^{N_{\text{mapped}}} \frac{\sum_{t=1}^{N_{\text{frames}}} \left( \frac{\left|G_{i}^{(t)} \cap D_{i}^{(t)}\right|}{\left|G_{i}^{(t)} \cup D_{i}^{(t)}\right|}  \right) }{ N_{(G_{i} \cup D_{i} \neq \emptyset)} }
\end{equation}
where $N_{\text{mapped}}$ refers to an optimal mapping between ground-truth and detected objects as specified in section \ref{sec:mappingtoground} using the STDA as the relevant metric, and $N_{(G_{i} \cup D_{i} \neq \emptyset)}$ denotes the number of frames in which a given tracked object, the ground truth object to which it is mapped, or both, are present.

Given the STDA for a video sequence, the ATA can be computed by the formula
\begin{equation}
\text{ATA} = \frac{\text{STDA}}{\left( \frac{N_{G} + N_{D}}{2} \right)}
\end{equation}

The ground-truth authored by the ViPER tool took the form of bounding boxes denoting the spatial postion of each object at each time step. Consequentially, to find the spatial overlap between results and ground-truth, which is intrinsic to both metrics, a rectangular bounding box was needed per object per time step from the results of the algorithm. We took the maximal and minimal axially aligned values of the oval produced by our algorithm (as described in Section~\ref{sec:inference_to_results}) to be the sides of a representative bounding box for a given object at a given frame.

\subsection{Synthetic Video Datasets}
\label{sec:syntheticvideos}

Synthetic videos provide controlled scenarios in which aspects of the technique presented in this paper can be tested. Each of the following synthetic videos consists of a sequence of 200 images (each of size $500$ x $500$ pixels) containing a number of smaller colored squares of different (and potentially time-varying) sizes moving at varied speeds and trajectories over a black background. The synthetic videos contain instances of occlusion (where one or more objects are briefly hidden) and objects with time-varying appearances and behaviors, as these are examples of occurances that notoriously decrease the accuracy of detection and tracking. After each video was constructed, the extraction procedure described in Section~\ref{sec:modelspec_extraction} (using $L=3$) and inference procedures described in \ref{sec:inference} were carried out to return a sequence of multivariate-normal-parameters (means and covariance matrices) from which a sequence of positions and ovals approximating the shape of a tracked object over each frame that it is present in the video can be determined (as outlined in Section~\ref{sec:inference_to_results}).


\subsubsection{Tracking by Appearance}

The first synthetic video experiment aimed to test the ability of the model and inference procedure to maintain the identity of independent objects based on color information alone. Two videos were constructed, both containing a red square (rgb value $[255,0,0]$ and size $20$ x $20$ pixels) and a blue square (rgb value $[0,0,255]$ and size $20$ x $20$ pixels). In both videos, the squares begin at opposite sides of the scene at frame $f=1$ and travel towards each other, arriving at the same location at $f=100$ (where the blue square occludes the red square). The second half of the two videos differ in that both squares in the first video continue in the same direction and end at the other's starting positions at $f=200$, and both squares in the second video reverse directions and end at their initial starting positions at $f=200$. Frame difference extraction on each video yields a dataset with identical spatial features; hence, successful tracking depends fully on the reasonable incorporation of color information into the model.

Parameters were set to the same values for inference on both videos, using the method outlined in Section~\ref{sec:parameter_recap} to choose parameters. The chosen values were $\alpha = 0.1, \rho = 0.3, M = 10, \boldsymbol{\mu}_{0} = (0,0), \kappa_{0} = 0.05, \nu_{0} = 5, \Lambda_{0} = \left( \begin{smallmatrix} 1&0\\ 0&1 \end{smallmatrix} \right), \text{ and } q_{0} = (5, \ldots, 5)$. Inference was carried out using the MCMC algorithm (Section~\ref{sec:MCMC}); the posterior sample taken as the result correctly tracked both colored squares through occlusion in both videos, and is shown in Figure~\ref{fig:synth_one_plot}. \willie{still need to include performance metric results for these synthetic datasets.}.


\begin{figure}
  \centering               
  \subfloat[]{\includegraphics[width=0.5\textwidth]{../img/cross_final}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{../img/bounce_final}}
  \caption{Each plot shows a sample from the posterior distribution of the state for one of the videos in synthetic experiment one, where the time axis ascends vertically, the horizontal axes represent spatial position, color represents assignment, and the mean and standard deviation are shown. In both cases, the two objects are successfully tracked through occlusion, though both objects reverse their directions in the second video. \willie{will show assignments by marker type in addition to color}}
  \label{fig:synth_one_plot}
\end{figure}




\subsubsection{Tracking though Occlusion and Appearance Shift}

The second synthetic video experiment aimed to test tracking performance under occlusion, object appearance change, and motion change. A video was constructed, containing a red square (rgb value $[255,0,0]$), a green square (rgb value $[0,255,0]$), and a blue square (rgb value $[0,0,255]$). The red square was of size $20$ x $20$ pixels, the blue square was of size $15$ x $15$ pixels, and the green square began at size $50$ x $50$ pixels at frame $f=1$, linearly shrinks to $10$ x $10$ pixels at $f=100$, then linearly grows back to $50$ x $50$ pixels by the end of the video, $f=200$. Furthermore, the red and blue squares display the same behavior as in the second video of the first synthetic experiment (they begin at opposite sides of the scene traveling towards each other, cross at the center of the scene at $f=100$, and reverse direction, ending at their initial positions at $f=200$). The green square begins at a point equidistant from the other two squares, intersects with them as they overlap (causing the blue square to occlude the other two), and continues on in a direction at a 20 degree angle from its initial trajectory.

Parameter values were chosen in the same way, and set to the same values, as the first synthetic experiment. The MCMC inference algorithm correctly tracked all three objects through occlusion and inferred the appearance and size shifts. Figure~\ref{fig:synth2_alphaimg} shows a sample from the posterior distribution of the cluster parameters, where the mean and oval representation of the covariance matrix (with $0.5$ confidence value) are overlayed on the data. The data is plotted in three dimensional space, where time is represented on the vertical axis, and each data point is colored by its assignment to one of the three inferred clusters. Both the cluster parameter sample and data are overlaid on three semi-transparent planes showing the objecs in the video in three frames. \willie{still need to include performance metric results for this synthetic dataset}.

% \begin{figure}
%   \centering               
%   % \subfloat[]{\includegraphics[width=0.35\textwidth, angle=30]{../img/synth2_img}}
%   \subfloat[]{\includegraphics[width=0.55\textwidth]{../img/synth2_img_b}}
%   \subfloat[]{\includegraphics[height=0.3\textheight]{../img/synth2_alphaimg_hd.png}}
%   \caption{(a.) Video frame $f=10$ from synthetic experiment two. (b.) Sample from posterior of the state for synthetic experiment two, where color represents assignment and the mean and standard deviation are shown. All are overlaid on three, time-varying images from synthetic experiment. The three objects of time varying size, speed, and position are tracked through simultaneous occlusion.  \willie{will show assignments by marker type in addition to color}}
%   \label{fig:synth_two_plot}
% \end{figure}

\begin{figure}[h]
        \center{\includegraphics[width=65mm]{../img/synth2_alphaimg_hd.png}}
        \caption{\label{fig:synth2_alphaimg} Sample from posterior of the state for synthetic experiment two, where color represents assignment and the mean and standard deviation are shown. Both the sample and data are overlaid on three images from synthetic experiment. The three objects of time varying size, speed, and position are tracked through simultaneous occlusion.  \willie{will show assignments by marker type in addition to color}}
\end{figure}

% \begin{figure}[h]
%         \center{\includegraphics[width=0.99\textwidth]{../img/synth2_alphaimg_hd.png}}
%         \caption{\label{fig:synth2_alphaimg} Extracted data and sample from the posterior of the model overlaid on three, time-varying images from synthetic experiment 2.}
% \end{figure}




\subsection{Benchmark Video Datasets}

Benchmark video datasets for object tracking and detection have been produced to provide standard scenes on which researchers can compare detection and tracking results. These videos have been primarily produced for surveillance-related workshops--notably, for the International Workshop on Performance Evaluation of Tracking and Surveillance (PETS)--which provide researchers with video datasets and algorithmic goals on which to focus. Three commonly used benchmark videos from PETS workshops--one used in PETS2000, one in PETS2001, and one used both in PETS2009 and PETS2010--were chosen to demonstrate the efficacy of the method presented in this study. The performance metrics and benchmark datasets allow the methods developed in this paper to be quantitatively compared against other detection and tracking algorithms.

%\subsubsection{PETS2000}

\subsubsection{PETS2000 and PETS2001}
\label{sec:pets2000_2001}

The PETS2000 and PETS2001 video datasets both consist of a small number of humans and vehicles traveling across a parking lot, with video taken from above, emulating what might be recorded by standard outdoor surveillance equipment. The `Test Sequence', a set of images from a monocular, stationary camera, was used from the PETS2000 workshop, and View Two of Dataset 1, also taken via a monocular, stationary camera, was used from the PETS2001 workshop. The MCMC algorithm (described in Section~\ref{sec:MCMC}) was used for inference in these experiments.

Due to the computation required for the MCMC batch inference method (discussed further in Section~\ref{sec:discussion}), only the final 1000 frames of the video were used. Extraction was performed with frame differencing as described in Section~\ref{sec:modelspec_extraction}, using $L=3$. Parameter values were chosen in the same way, and set to the same values, as in the the synthetic experiments ($\alpha = 0.1, \rho = 0.3, M = 10, \boldsymbol{\mu}_{0} = (0,0), \kappa_{0} = 0.05, \nu_{0} = 5, \Lambda_{0} = \left( \begin{smallmatrix} 1&0\\ 0&1 \end{smallmatrix} \right), \text{ and } q_{0} = (5, \ldots, 5)$). The MCMC sampler was successful for both benchmark videos; each object was detected, tracked, and its shape estimated in manner very consistent with the ground-truth. The results for the PETS2000 dataset are displayed in Figure~\ref{fig:pets2000_results} and for the PETS2001 dataset in Figure~\ref{fig:pets2001_alphaimg}; in these figure, a sample from the posterior of the cluster parameters is overlayed the data and video frames at three points in time, where the data is colored by assignment. \willie{also show pets 2001 result and per-frame overlays for both}.

\begin{figure}[h]
        \center{\includegraphics[width=65mm]{../img/pets2001_alphaimg_hr.png}}
        \caption{\label{fig:pets2001_alphaimg} Extracted data and sample from posterior of the model overlaid on three frames from the PETS2001 video.}
\end{figure}

\begin{figure}[h]
	\centering
	\subfloat[]{\includegraphics[width=0.58\textwidth]{../img/pets2000_results_1}}
	\subfloat[]{\includegraphics[width=0.42\textwidth]{../img/pets2000_4objects.png}}
	\caption{Extracted data and a sample from posterior of the model (a) overlaid on a frame from the PETS2000 video (b).}
	\label{fig:pets2000_results}
\end{figure}

To calculate performance metrics (both SFDA and ATA), one must specify a confidence value that allows the oval representing the region occupied by an object to be computed from the inferred covariance matrix of each cluster (as discussed in Section~\ref{sec:inference_to_results}). The performance metrics were found for a range of confidence intervals, and these curves, for the PETS2000 and PETS2001 video are shown in Figure~\ref{fig:pm_conf}.


\begin{figure}[h]
  \centering             
  \subfloat[]{\includegraphics[width=0.5\textwidth]{../img/pets2000_pm}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{../img/pets2001_pm}}
  \caption{The SFDA (blue solid line) and ATA (red dashed line are) vs  confidence values from which an object's oval region is computed for (a) PETS2000 and (b) PETS2001 video datasets.}
  \label{fig:pm_conf}
\end{figure}

\begin{figure}[h]
  \centering             
  \subfloat[]{\includegraphics[width=0.25\textwidth]{../img/pets2001_1}}
  \subfloat[]{\includegraphics[width=0.25\textwidth]{../img/pets2001_2}}
  \subfloat[]{\includegraphics[width=0.25\textwidth]{../img/pets2001_3}}
  \subfloat[]{\includegraphics[width=0.25\textwidth]{../img/pets2001_4}}
  \caption{Four frames $f=25, 35, 45, 55$ from the PETS2001 sequence with one posterior sample mean and covarance matrix overlaid. \willie{will show path trail over prev 10 frames.} \willie{These are old images!... haven't replaced yet}}
  \label{fig:pets2001_imgs}
\end{figure}




\subsubsection{PETS2009/2010}

A video dataset used in both the PETS2009 and PETS2010 conferences, called `S2.L1 at time sequence 12.34' was chosen for experimentation due to its prominence in a number of studies \willie{ref papers using this dataset}. This dataset consists of a monocular, stationary camera, 794 frame video sequence. The entire video sequence was used in this experiment. 

Due to the large number of frames and objects in this video, the SMC algorithm (described in Section~\ref{sec:SMC}) was used for inference. This method of sequential inference was observed, on this dataset, to converge to a better sample in a shorter period of time in comparison with the MCMC algorithm; for this reason, we found that it was better suited for this large dataset.

Extraction was performed with frame differencing as described in Section~\ref{sec:modelspec_extraction}, using $L=3$ and parameters for the model were chosen, using the method outlined in Section~\ref{sec:parameter_recap}, to be $\alpha = 0.1, \rho = 0.8, M = 10, \boldsymbol{\mu}_{0} = (0,0), \kappa_{0} = 0.05, \nu_{0} = 6, \Lambda_{0} = \left( \begin{smallmatrix} 1&0\\ 0&1 \end{smallmatrix} \right), \text{ and } q_{0} = (3, \ldots, 3)$. Additionally, as with the other video datasets, ground-truth bounding boxes around each object were authored using the ViPER tool.

The SMC inference algorithm yielded a sample from the posterior distribution, from which the object detection and tracking results were obtained (as described in Section~\ref{sec:inference_to_results}). Figure \willie{provide figure showing PETS2009 results} shows a sample from the posterior of the cluster parameters overlayed on the data, and video frames at three points in time, where the data is colored by assignment
\willie{show SFDA and ATA vs cov mat conf for this sample that I show figures for.}




\subsubsection{Sensitivity Analysis}

SMC inference on the PETS2009 video dataset was carried out for a range of two key parameter values, $\alpha$ and $\rho$. The performance metric measures, SFDA and ATA, were computed for each combination of these two parameters. This sensitivity investigation focused on these parameters due to their potential to have a large effect on object detection accuracies. The $\alpha$ values tested included $\{ 0.01, 0.1, 1, 10, 100 \}$, and the $\rho$ values tested included $\{ 0.7, 0.75, 0.8, 0.85, 0.9 \}$.

The following shows the SFDA sensitivity analysis results. The largest value is in bold.
\begin{center}
\begin{tabular}[c]{l l  l  l  l  l  l}
  & & & $\hspace{8mm}\rho$ & & & \\
  SFDA  &  	& $0.7$ & $0.75$ & $0.8$ & $0.85$ & $0.9$  \\  
  &$0.01$ 	& $0.4121$ & $0.4262$ & $0.4219$ & $\bold{0.4331}$ & $0.4277$  \\
  &$0.1$  	& $\bold{0.4336}$ & $0.4184$ & $0.4200$ & $0.4316$ & $0.4321$  \\ 
$\alpha$ 	& $1$    & $0.4084$ & $0.4209$ & $0.4205$ & $0.4290$ & $0.4330$  \\  
  &$10$   	& $0.3992$ & $0.4130$ & $0.4289$ & $0.4232$ & $0.4226$  \\  
  &$100$  	& $0.3655$ & $0.3739$ & $0.3636$ & $0.3755$ & $0.3672$  \\
\end{tabular}
\end{center}

The following shows the ATA sensitivity analysis results. The largest value is in bold.
\begin{center}
\begin{tabular}[c]{l l  l  l  l  l  l}
		& & & $\hspace{8mm}\rho$ & & & \\
 	 ATA  & 	 	& $0.7$ & $0.75$ & $0.8$ & $0.85$ & $0.9$  \\  
	  &$0.01$ 		& $0.2032$ & $0.2017$ & $0.2262$ & $0.1945$ & $0.1914$  \\
		  &$0.1$ 	& $0.1964$ & $0.2042$ & $0.2401$ & $0.1990$ & $0.2017$  \\
$\alpha$ & $1$  	& $0.2016$ & $0.2365$ & $0.2766$ & $0.2347$ & $0.2766$  \\   
  		&$10$   	& $0.2277$ & $0.2284$ & $0.2613$ & $\bold{0.2781}$ & $0.2732$  \\  
 		 &$100$ 	& $0.2468$ & $0.2645$ & $0.2725$ & $0.2670$ & $\bold{0.2843}$  \\
\end{tabular}
\end{center}


% By inspecting the figures, one can see that there is a point where the SFDA and ATA both achieve a maximal value.



\begin{figure}[h]
  \centering             
  \subfloat[]{\includegraphics[width=0.5\textwidth]{../img/pets2009full_sens_sfda.png}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{../img/pets2009full_sens_ata.png}}
  \caption{Surface plot of the sensitivity performance metric tables}
  \label{fig:sens_surf}
\end{figure}




% \begin{figure}
%   \centering             
%   \subfloat[]{\includegraphics[width=0.33\textwidth]{../img/pets2009_result1.png}}
%   \subfloat[]{\includegraphics[width=0.33\textwidth]{../img/pets2009_result2.png}}
%   \subfloat[]{\includegraphics[width=0.33\textwidth]{../img/pets2009_result3.png}}
%   \caption{Surface of the sensitivity performance metric tables}
%   \label{fig:sens_surf}
% \end{figure}


% \begin{figure}
%   \centering             
%   \subfloat[]{\includegraphics[width=0.25\textwidth]{../img/pets2009_1}}
%   \subfloat[]{\includegraphics[width=0.25\textwidth]{../img/pets2009_2}}
%   \subfloat[]{\includegraphics[width=0.25\textwidth]{../img/pets2009_3}}
%   \subfloat[]{\includegraphics[width=0.25\textwidth]{../img/pets2009_4}}
%   \caption{Four frames $f=10, 25, 35, 75$ from the PETS2009 sequence with one posterior sample mean and covarance matrix overlaid. \willie{will show path trail over prev 10 frames}\willie{these are old images!... haven't replaced yet}}
%   \label{fig:pets2009_imgs}
% \end{figure}


% \begin{figure}
% 	\centering
% 	\includegraphics[width=3in]{../img/pets2009_results}
% 	% \subfloat[]{\includegraphics[width=0.52\textwidth]{../img/pets2001_results_1}}
% 	% \subfloat[]{\includegraphics[width=0.48\textwidth]{../img/pets2001_results_2}}
% 	\caption{Sample from posterior of the state for the PETS2009 dataset, where color represents assignment and the mean and standard deviation are shown. \willie{will show assignments by marker type in addition to color}}
% 	\label{fig:pets2009_results}
% \end{figure}

% \subsection{New Video Datasets}

% The two video datasets in this section are video datasets first used in this paper. The datasets and groundtruth detailing the positions of each obect in each video will be published online.

% \subsubsection{Ants Video?}

% \subsubsection{Cells Video?}




\section*{Discussion}
\label{sec:discussion}

Discuss parameters somewhere in here. to add in discussion: thoughts about: deletion param vs. dataset size (and tracking through occlusion vs clusters taking over), mvn cluster size vs. tracking in occlusion vs. sampling correct number of objects.  we detect movement, which isnt exactly same as perf. metrics (ie our tracking stops if people stop moving). objects hard to track through occlusion becuase they are small, similar appearance, and our algorithm doesnt assume dynamics to keep robust (we could, probably for better accuracy but less generality). background (appearance) is like a natural prior for remaining stationary




\section*{Conclusion}















\begin{small}
\bibliographystyle{plainnat}
\bibliography{paper_refs} 
\end{small}



\end{document}